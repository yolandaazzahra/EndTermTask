{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkPzDjxAp8kEoJNAKcjx7d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yolandaazzahra/EndTermTask/blob/main/EndTermTask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nama : Yolanda Azzahra\n",
        "\n",
        "NPM : 2306319514\n"
      ],
      "metadata": {
        "id": "GzWz9PqtbYvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 11 : GAN"
      ],
      "metadata": {
        "id": "h9AX_e3HmDn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.2"
      ],
      "metadata": {
        "id": "0nz9BKl6bWhW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl0D6ZJca8Ly",
        "outputId": "ec75cbc1-2036-466a-fbf0-6c28fb1a2a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 493kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.47MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Epoch [1/5], Batch [0/938], Critic Loss: 1.4276, Generator Loss: 0.6950\n",
            "Epoch [1/5], Batch [100/938], Critic Loss: 1.3980, Generator Loss: 0.5792\n",
            "Epoch [1/5], Batch [200/938], Critic Loss: 0.7749, Generator Loss: 0.9953\n",
            "Epoch [1/5], Batch [300/938], Critic Loss: 0.3529, Generator Loss: 1.7672\n",
            "Epoch [1/5], Batch [400/938], Critic Loss: 0.7642, Generator Loss: 1.0124\n",
            "Epoch [1/5], Batch [500/938], Critic Loss: 0.8313, Generator Loss: 1.1024\n",
            "Epoch [1/5], Batch [600/938], Critic Loss: 1.0143, Generator Loss: 0.9892\n",
            "Epoch [1/5], Batch [700/938], Critic Loss: 0.7459, Generator Loss: 1.3113\n",
            "Epoch [1/5], Batch [800/938], Critic Loss: 0.6891, Generator Loss: 1.4761\n",
            "Epoch [1/5], Batch [900/938], Critic Loss: 0.6674, Generator Loss: 1.4291\n",
            "Epoch [2/5], Batch [0/938], Critic Loss: 1.1677, Generator Loss: 0.9664\n",
            "Epoch [2/5], Batch [100/938], Critic Loss: 0.7216, Generator Loss: 1.3709\n",
            "Epoch [2/5], Batch [200/938], Critic Loss: 0.8392, Generator Loss: 1.2067\n",
            "Epoch [2/5], Batch [300/938], Critic Loss: 1.3301, Generator Loss: 0.7691\n",
            "Epoch [2/5], Batch [400/938], Critic Loss: 1.0128, Generator Loss: 1.0926\n",
            "Epoch [2/5], Batch [500/938], Critic Loss: 0.7625, Generator Loss: 1.3991\n",
            "Epoch [2/5], Batch [600/938], Critic Loss: 0.7180, Generator Loss: 1.4669\n",
            "Epoch [2/5], Batch [700/938], Critic Loss: 0.8944, Generator Loss: 1.2987\n",
            "Epoch [2/5], Batch [800/938], Critic Loss: 0.6650, Generator Loss: 1.5803\n",
            "Epoch [2/5], Batch [900/938], Critic Loss: 0.4882, Generator Loss: 1.6665\n",
            "Epoch [3/5], Batch [0/938], Critic Loss: 1.1047, Generator Loss: 0.9996\n",
            "Epoch [3/5], Batch [100/938], Critic Loss: 0.7233, Generator Loss: 1.2848\n",
            "Epoch [3/5], Batch [200/938], Critic Loss: 1.4044, Generator Loss: 0.8819\n",
            "Epoch [3/5], Batch [300/938], Critic Loss: 0.5161, Generator Loss: 1.7102\n",
            "Epoch [3/5], Batch [400/938], Critic Loss: 0.4716, Generator Loss: 1.8002\n",
            "Epoch [3/5], Batch [500/938], Critic Loss: 0.6653, Generator Loss: 1.7646\n",
            "Epoch [3/5], Batch [600/938], Critic Loss: 0.5474, Generator Loss: 1.7128\n",
            "Epoch [3/5], Batch [700/938], Critic Loss: 0.5588, Generator Loss: 1.6392\n",
            "Epoch [3/5], Batch [800/938], Critic Loss: 0.8796, Generator Loss: 1.4037\n",
            "Epoch [3/5], Batch [900/938], Critic Loss: 0.5818, Generator Loss: 1.7638\n",
            "Epoch [4/5], Batch [0/938], Critic Loss: 0.4249, Generator Loss: 1.9362\n",
            "Epoch [4/5], Batch [100/938], Critic Loss: 0.4445, Generator Loss: 2.2576\n",
            "Epoch [4/5], Batch [200/938], Critic Loss: 0.7813, Generator Loss: 1.8262\n",
            "Epoch [4/5], Batch [300/938], Critic Loss: 0.3788, Generator Loss: 2.6432\n",
            "Epoch [4/5], Batch [400/938], Critic Loss: 0.3297, Generator Loss: 2.6178\n",
            "Epoch [4/5], Batch [500/938], Critic Loss: 0.2927, Generator Loss: 2.3240\n",
            "Epoch [4/5], Batch [600/938], Critic Loss: 0.4710, Generator Loss: 2.1146\n",
            "Epoch [4/5], Batch [700/938], Critic Loss: 0.2582, Generator Loss: 2.6893\n",
            "Epoch [4/5], Batch [800/938], Critic Loss: 0.2349, Generator Loss: 3.3160\n",
            "Epoch [4/5], Batch [900/938], Critic Loss: 0.3810, Generator Loss: 2.6953\n",
            "Epoch [5/5], Batch [0/938], Critic Loss: 0.4074, Generator Loss: 2.4478\n",
            "Epoch [5/5], Batch [100/938], Critic Loss: 0.3309, Generator Loss: 2.9991\n",
            "Epoch [5/5], Batch [200/938], Critic Loss: 0.2311, Generator Loss: 3.0379\n",
            "Epoch [5/5], Batch [300/938], Critic Loss: 0.1925, Generator Loss: 2.6452\n",
            "Epoch [5/5], Batch [400/938], Critic Loss: 0.3053, Generator Loss: 2.4862\n",
            "Epoch [5/5], Batch [500/938], Critic Loss: 0.2847, Generator Loss: 2.5417\n",
            "Epoch [5/5], Batch [600/938], Critic Loss: 0.3700, Generator Loss: 2.4924\n",
            "Epoch [5/5], Batch [700/938], Critic Loss: 0.3310, Generator Loss: 2.2680\n",
            "Epoch [5/5], Batch [800/938], Critic Loss: 0.1370, Generator Loss: 3.3633\n",
            "Epoch [5/5], Batch [900/938], Critic Loss: 0.1234, Generator Loss: 3.7872\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Load MNIST dataset and normalize pixel values to [0, 1]\n",
        "def load_mnist_data(data_dir=\"data\"):\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    train_dataset = datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "    return train_loader\n",
        "\n",
        "# Build the generator network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 28 * 28),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "# Build the critic/discriminator network\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Critic, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Train Minimax GAN\n",
        "def train_minimax_gan(train_loader, epochs, batch_size, latent_dim, learning_rate, device):\n",
        "    generator = Generator(latent_dim).to(device)\n",
        "    critic = Critic().to(device)\n",
        "\n",
        "    opt_gen = optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "    opt_critic = optim.Adam(critic.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for batch_idx, (real_images, _) in enumerate(train_loader):\n",
        "            # Pastikan batch memiliki ukuran yang sesuai\n",
        "            current_batch_size = real_images.size(0)  # Ambil ukuran aktual dari batch saat ini\n",
        "            real_images = real_images.view(current_batch_size, -1).to(device)\n",
        "\n",
        "            # Train the critic\n",
        "            noise = torch.randn(current_batch_size, latent_dim).to(device)  # Gunakan ukuran batch yang sesuai\n",
        "            fake_images = generator(noise).detach()\n",
        "            real_score = torch.sigmoid(critic(real_images))\n",
        "            fake_score = torch.sigmoid(critic(fake_images))\n",
        "            critic_loss = -torch.mean(torch.log(real_score + 1e-8) + torch.log(1 - fake_score + 1e-8))\n",
        "\n",
        "            opt_critic.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            opt_critic.step()\n",
        "\n",
        "            # Train the generator\n",
        "            noise = torch.randn(current_batch_size, latent_dim).to(device)  # Gunakan ukuran batch yang sesuai\n",
        "            fake_images = generator(noise)\n",
        "            fake_score = torch.sigmoid(critic(fake_images))\n",
        "            generator_loss = -torch.mean(torch.log(fake_score + 1e-8))\n",
        "\n",
        "            opt_gen.zero_grad()\n",
        "            generator_loss.backward()\n",
        "            opt_gen.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(\n",
        "                    f\"Epoch [{epoch}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], \"\n",
        "                    f\"Critic Loss: {critic_loss.item():.4f}, Generator Loss: {generator_loss.item():.4f}\"\n",
        "                )\n",
        "\n",
        "\n",
        "# Generate and save images\n",
        "def generate_and_save_images(generator, epoch, device, output_dir=\"output\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    generator.eval()\n",
        "    noise = torch.randn(64, 100).to(device)\n",
        "    fake_images = generator(noise).view(-1, 28, 28).cpu().detach().numpy()\n",
        "    fake_images = (fake_images + 1) / 2.0 * 255.0  # Rescale to [0, 255]\n",
        "\n",
        "    for i, img in enumerate(fake_images):\n",
        "        img = Image.fromarray(img.astype(np.uint8), mode='L')\n",
        "        img.save(os.path.join(output_dir, f\"epoch{epoch}_img{i}.png\"))\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    train_loader = load_mnist_data()\n",
        "    epochs = 5\n",
        "    batch_size = 64\n",
        "    latent_dim = 100\n",
        "    learning_rate = 0.00005\n",
        "\n",
        "    train_minimax_gan(train_loader, epochs, batch_size, latent_dim, learning_rate, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "# Generator Network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 28 * 28),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "\n",
        "# Critic Network\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Critic, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Function to train WGAN\n",
        "def train_wgan(\n",
        "    train_loader, epochs, batch_size, latent_dim, learning_rate, critic_iterations, clipping_value, device\n",
        "):\n",
        "    generator = Generator(latent_dim).to(device)\n",
        "    critic = Critic().to(device)\n",
        "\n",
        "    opt_gen = optim.RMSprop(generator.parameters(), lr=learning_rate)\n",
        "    opt_critic = optim.RMSprop(critic.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for batch_idx, (real_images, _) in enumerate(train_loader):\n",
        "            real_images = real_images.view(-1, 28 * 28).to(device)\n",
        "            current_batch_size = real_images.size(0)\n",
        "\n",
        "            # Train the critic\n",
        "            for _ in range(critic_iterations):\n",
        "                noise = torch.randn(current_batch_size, latent_dim).to(device)\n",
        "                fake_images = generator(noise).detach()\n",
        "\n",
        "                real_score = critic(real_images)\n",
        "                fake_score = critic(fake_images)\n",
        "\n",
        "                critic_loss = fake_score.mean() - real_score.mean()\n",
        "\n",
        "                opt_critic.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                opt_critic.step()\n",
        "\n",
        "                # Weight clipping\n",
        "                for p in critic.parameters():\n",
        "                    p.data.clamp_(-clipping_value, clipping_value)\n",
        "\n",
        "            # Train the generator\n",
        "            noise = torch.randn(current_batch_size, latent_dim).to(device)\n",
        "            fake_images = generator(noise)\n",
        "            fake_score = critic(fake_images)\n",
        "\n",
        "            generator_loss = -fake_score.mean()\n",
        "\n",
        "            opt_gen.zero_grad()\n",
        "            generator_loss.backward()\n",
        "            opt_gen.step()\n",
        "\n",
        "            # Logging\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(\n",
        "                    f\"Epoch [{epoch}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], \"\n",
        "                    f\"Critic Loss: {critic_loss.item():.4f}, Generator Loss: {generator_loss.item():.4f}\"\n",
        "                )\n",
        "\n",
        "        # Save generated images at the end of each epoch\n",
        "        output_dir = \"output_wgan\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        generate_and_save_images(generator, latent_dim, epoch, device, output_dir)\n",
        "\n",
        "\n",
        "# Function to generate and save images\n",
        "def generate_and_save_images(generator, latent_dim, epoch, device, output_dir):\n",
        "    noise = torch.randn(64, latent_dim).to(device)\n",
        "    fake_images = generator(noise).view(-1, 1, 28, 28).cpu()\n",
        "    fake_images = (fake_images + 1) / 2.0  # Rescale images to [0, 1]\n",
        "    save_image(fake_images, os.path.join(output_dir, f\"epoch_{epoch}.png\"), nrow=8)\n",
        "\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    mnist = datasets.MNIST(root=\"data\", train=True, transform=transform, download=True)\n",
        "    train_loader = torch.utils.data.DataLoader(mnist, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Training parameters\n",
        "    epochs = 5\n",
        "    batch_size = 64\n",
        "    latent_dim = 100\n",
        "    learning_rate = 0.00005\n",
        "    critic_iterations = 5\n",
        "    clipping_value = 0.01\n",
        "\n",
        "    # Train WGAN\n",
        "    print(\"Training WGAN...\")\n",
        "    train_wgan(train_loader, epochs, batch_size, latent_dim, learning_rate, critic_iterations, clipping_value, device)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMZrePKWdZjG",
        "outputId": "ccc70ade-2e8e-4c6b-ffe9-cc87221fc186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training WGAN...\n",
            "Epoch [1/5], Batch [0/938], Critic Loss: -0.1005, Generator Loss: 0.0064\n",
            "Epoch [1/5], Batch [100/938], Critic Loss: -0.0959, Generator Loss: -2.0477\n",
            "Epoch [1/5], Batch [200/938], Critic Loss: -0.1890, Generator Loss: -0.2643\n",
            "Epoch [1/5], Batch [300/938], Critic Loss: -0.3985, Generator Loss: -1.7027\n",
            "Epoch [1/5], Batch [400/938], Critic Loss: -0.4744, Generator Loss: -2.2196\n",
            "Epoch [1/5], Batch [500/938], Critic Loss: -0.4205, Generator Loss: -2.2928\n",
            "Epoch [1/5], Batch [600/938], Critic Loss: -0.3910, Generator Loss: -2.4727\n",
            "Epoch [1/5], Batch [700/938], Critic Loss: -0.5246, Generator Loss: -1.9415\n",
            "Epoch [1/5], Batch [800/938], Critic Loss: -0.4729, Generator Loss: -1.5136\n",
            "Epoch [1/5], Batch [900/938], Critic Loss: -0.4249, Generator Loss: -1.2204\n",
            "Epoch [2/5], Batch [0/938], Critic Loss: -0.5361, Generator Loss: -1.3954\n",
            "Epoch [2/5], Batch [100/938], Critic Loss: -0.5058, Generator Loss: -1.1842\n",
            "Epoch [2/5], Batch [200/938], Critic Loss: -0.3254, Generator Loss: -1.3290\n",
            "Epoch [2/5], Batch [300/938], Critic Loss: -0.5442, Generator Loss: -1.0543\n",
            "Epoch [2/5], Batch [400/938], Critic Loss: -0.5184, Generator Loss: -0.4548\n",
            "Epoch [2/5], Batch [500/938], Critic Loss: -0.4949, Generator Loss: 0.1856\n",
            "Epoch [2/5], Batch [600/938], Critic Loss: -0.4346, Generator Loss: 0.1120\n",
            "Epoch [2/5], Batch [700/938], Critic Loss: -0.3614, Generator Loss: 0.7917\n",
            "Epoch [2/5], Batch [800/938], Critic Loss: -0.6372, Generator Loss: 0.8495\n",
            "Epoch [2/5], Batch [900/938], Critic Loss: -0.6291, Generator Loss: 0.3427\n",
            "Epoch [3/5], Batch [0/938], Critic Loss: -0.6258, Generator Loss: -0.0536\n",
            "Epoch [3/5], Batch [100/938], Critic Loss: -0.6182, Generator Loss: 0.5555\n",
            "Epoch [3/5], Batch [200/938], Critic Loss: -0.7153, Generator Loss: 0.4333\n",
            "Epoch [3/5], Batch [300/938], Critic Loss: -0.9502, Generator Loss: 1.0093\n",
            "Epoch [3/5], Batch [400/938], Critic Loss: -0.8943, Generator Loss: 1.2409\n",
            "Epoch [3/5], Batch [500/938], Critic Loss: -0.8614, Generator Loss: 0.9242\n",
            "Epoch [3/5], Batch [600/938], Critic Loss: -1.0310, Generator Loss: 0.8228\n",
            "Epoch [3/5], Batch [700/938], Critic Loss: -0.8163, Generator Loss: 0.6815\n",
            "Epoch [3/5], Batch [800/938], Critic Loss: -0.8714, Generator Loss: 0.9478\n",
            "Epoch [3/5], Batch [900/938], Critic Loss: -0.9476, Generator Loss: 0.8346\n",
            "Epoch [4/5], Batch [0/938], Critic Loss: -0.8851, Generator Loss: 0.8663\n",
            "Epoch [4/5], Batch [100/938], Critic Loss: -0.9105, Generator Loss: 1.1491\n",
            "Epoch [4/5], Batch [200/938], Critic Loss: -1.0123, Generator Loss: 1.4240\n",
            "Epoch [4/5], Batch [300/938], Critic Loss: -0.9090, Generator Loss: 0.7290\n",
            "Epoch [4/5], Batch [400/938], Critic Loss: -0.9016, Generator Loss: 1.1628\n",
            "Epoch [4/5], Batch [500/938], Critic Loss: -0.9824, Generator Loss: 0.8292\n",
            "Epoch [4/5], Batch [600/938], Critic Loss: -0.9702, Generator Loss: 1.3029\n",
            "Epoch [4/5], Batch [700/938], Critic Loss: -0.9527, Generator Loss: 0.8606\n",
            "Epoch [4/5], Batch [800/938], Critic Loss: -1.0139, Generator Loss: 1.0406\n",
            "Epoch [4/5], Batch [900/938], Critic Loss: -1.1284, Generator Loss: 0.5355\n",
            "Epoch [5/5], Batch [0/938], Critic Loss: -1.0607, Generator Loss: 1.3054\n",
            "Epoch [5/5], Batch [100/938], Critic Loss: -1.0326, Generator Loss: 1.1407\n",
            "Epoch [5/5], Batch [200/938], Critic Loss: -0.9997, Generator Loss: 1.1230\n",
            "Epoch [5/5], Batch [300/938], Critic Loss: -0.9501, Generator Loss: 1.1957\n",
            "Epoch [5/5], Batch [400/938], Critic Loss: -1.1603, Generator Loss: 1.0853\n",
            "Epoch [5/5], Batch [500/938], Critic Loss: -0.8982, Generator Loss: 0.8040\n",
            "Epoch [5/5], Batch [600/938], Critic Loss: -1.1193, Generator Loss: 1.1527\n",
            "Epoch [5/5], Batch [700/938], Critic Loss: -0.9446, Generator Loss: 1.2869\n",
            "Epoch [5/5], Batch [800/938], Critic Loss: -1.1291, Generator Loss: 1.1161\n",
            "Epoch [5/5], Batch [900/938], Critic Loss: -1.0435, Generator Loss: 1.0376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "# Konstanta\n",
        "IMG_SIZE = 64\n",
        "LATENT_DIM = 128\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCHES = 10000\n",
        "\n",
        "# Dataset Custom untuk Memuat Gambar\n",
        "class UploadedImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# Fungsi untuk Memuat Data dari Gambar yang Diunggah\n",
        "def load_uploaded_images(uploaded_files, img_size=64, batch_size=32):\n",
        "    image_paths = []\n",
        "    for filename in uploaded_files.keys():\n",
        "        image_paths.append(filename)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "\n",
        "    dataset = UploadedImageDataset(image_paths, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "# Model Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(LATENT_DIM, 1024, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Model Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1024, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(1024, 1, 4, 1, 0, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Fungsi Pelatihan\n",
        "def train_gan(generator, discriminator, dataloader, device):\n",
        "    opt_g = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "    criterion = nn.MSELoss()\n",
        "    fixed_noise = torch.randn(BATCH_SIZE, LATENT_DIM, 1, 1, device=device)\n",
        "\n",
        "    for epoch in range(BATCHES):\n",
        "        for real_images in dataloader:\n",
        "            real_images = real_images.to(device)\n",
        "\n",
        "            # Pelatihan Discriminator\n",
        "            discriminator.zero_grad()\n",
        "            real_labels = torch.ones(real_images.size(0), 1, device=device)\n",
        "            fake_labels = torch.zeros(real_images.size(0), 1, device=device)\n",
        "            real_output = discriminator(real_images).view(-1)\n",
        "            d_loss_real = criterion(real_output, real_labels)\n",
        "\n",
        "            noise = torch.randn(real_images.size(0), LATENT_DIM, 1, 1, device=device)\n",
        "            fake_images = generator(noise)\n",
        "            fake_output = discriminator(fake_images.detach()).view(-1)\n",
        "            d_loss_fake = criterion(fake_output, fake_labels)\n",
        "\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "            d_loss.backward()\n",
        "            opt_d.step()\n",
        "\n",
        "            # Pelatihan Generator\n",
        "            generator.zero_grad()\n",
        "            fake_output = discriminator(fake_images).view(-1)\n",
        "            g_loss = criterion(fake_output, real_labels)\n",
        "            g_loss.backward()\n",
        "            opt_g.step()\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch [{epoch}/{BATCHES}]  Loss D: {d_loss.item()}, Loss G: {g_loss.item()}\")\n",
        "\n",
        "# Main Program\n",
        "if __name__ == \"__main__\":\n",
        "    # Upload gambar menggunakan Google Colab\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Load gambar dari upload\n",
        "    dataloader = load_uploaded_images(uploaded, img_size=IMG_SIZE, batch_size=BATCH_SIZE)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Inisialisasi Model\n",
        "    generator = Generator().to(device)\n",
        "    discriminator = Discriminator().to(device)\n",
        "\n",
        "    # Latih GAN\n",
        "    train_gan(generator, discriminator, dataloader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JBIiiphyiQOj",
        "outputId": "9d123e20-a66a-4cff-93a7-5a0301848527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ed45cdde-9fb2-4d8d-bcb6-515e4db1c56a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ed45cdde-9fb2-4d8d-bcb6-515e4db1c56a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving person1_virus_6.jpeg to person1_virus_6.jpeg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/10000]  Loss D: 0.25707942247390747, Loss G: 304.6692199707031\n",
            "Epoch [100/10000]  Loss D: 0.5128493905067444, Loss G: 4.641046524047852\n",
            "Epoch [200/10000]  Loss D: 0.09020882099866867, Loss G: 0.6867765784263611\n",
            "Epoch [300/10000]  Loss D: 0.8848155736923218, Loss G: 0.11726993322372437\n",
            "Epoch [400/10000]  Loss D: 0.4400639235973358, Loss G: 3.4583518505096436\n",
            "Epoch [500/10000]  Loss D: 0.011034689843654633, Loss G: 1.5810446739196777\n",
            "Epoch [600/10000]  Loss D: 0.8594956398010254, Loss G: 6.331902503967285\n",
            "Epoch [700/10000]  Loss D: 0.04018475487828255, Loss G: 1.5776571035385132\n",
            "Epoch [800/10000]  Loss D: 0.4033131003379822, Loss G: 0.31742456555366516\n",
            "Epoch [900/10000]  Loss D: 0.05708761513233185, Loss G: 0.452210009098053\n",
            "Epoch [1000/10000]  Loss D: 0.05089739337563515, Loss G: 1.3969789743423462\n",
            "Epoch [1100/10000]  Loss D: 0.024989277124404907, Loss G: 1.2501888275146484\n",
            "Epoch [1200/10000]  Loss D: 0.0311281718313694, Loss G: 0.7172957062721252\n",
            "Epoch [1300/10000]  Loss D: 0.12548692524433136, Loss G: 1.0975372791290283\n",
            "Epoch [1400/10000]  Loss D: 0.3090296685695648, Loss G: 2.3101282119750977\n",
            "Epoch [1500/10000]  Loss D: 0.04785805195569992, Loss G: 0.8991571664810181\n",
            "Epoch [1600/10000]  Loss D: 0.0074074044823646545, Loss G: 0.9511243104934692\n",
            "Epoch [1700/10000]  Loss D: 0.11285664141178131, Loss G: 0.45398715138435364\n",
            "Epoch [1800/10000]  Loss D: 0.027370713651180267, Loss G: 1.1364954710006714\n",
            "Epoch [1900/10000]  Loss D: 0.016125107184052467, Loss G: 1.1072252988815308\n",
            "Epoch [2000/10000]  Loss D: 0.13893117010593414, Loss G: 1.383632779121399\n",
            "Epoch [2100/10000]  Loss D: 0.08916263282299042, Loss G: 1.0174362659454346\n",
            "Epoch [2200/10000]  Loss D: 0.01971575990319252, Loss G: 0.9972071051597595\n",
            "Epoch [2300/10000]  Loss D: 0.31454139947891235, Loss G: 1.4394733905792236\n",
            "Epoch [2400/10000]  Loss D: 0.012428965419530869, Loss G: 1.2628276348114014\n",
            "Epoch [2500/10000]  Loss D: 0.016499962657690048, Loss G: 0.75937420129776\n",
            "Epoch [2600/10000]  Loss D: 0.004334175493568182, Loss G: 0.8553792238235474\n",
            "Epoch [2700/10000]  Loss D: 0.01236823108047247, Loss G: 0.7930468320846558\n",
            "Epoch [2800/10000]  Loss D: 0.11619409918785095, Loss G: 0.5360020399093628\n",
            "Epoch [2900/10000]  Loss D: 0.003964715637266636, Loss G: 0.9035596251487732\n",
            "Epoch [3000/10000]  Loss D: 0.00195630663074553, Loss G: 0.9881406426429749\n",
            "Epoch [3100/10000]  Loss D: 0.00889057107269764, Loss G: 1.0703880786895752\n",
            "Epoch [3200/10000]  Loss D: 0.026586126536130905, Loss G: 0.7814205884933472\n",
            "Epoch [3300/10000]  Loss D: 0.00734221376478672, Loss G: 1.0517951250076294\n",
            "Epoch [3400/10000]  Loss D: 0.0007649987819604576, Loss G: 1.0465153455734253\n",
            "Epoch [3500/10000]  Loss D: 0.08445116877555847, Loss G: 0.7963303327560425\n",
            "Epoch [3600/10000]  Loss D: 0.17738592624664307, Loss G: 1.6671298742294312\n",
            "Epoch [3700/10000]  Loss D: 0.006376954726874828, Loss G: 1.1985342502593994\n",
            "Epoch [3800/10000]  Loss D: 0.08545099198818207, Loss G: 1.7629162073135376\n",
            "Epoch [3900/10000]  Loss D: 0.08030672371387482, Loss G: 1.7085641622543335\n",
            "Epoch [4000/10000]  Loss D: 0.35471779108047485, Loss G: 1.8106955289840698\n",
            "Epoch [4100/10000]  Loss D: 0.012135609984397888, Loss G: 1.16274094581604\n",
            "Epoch [4200/10000]  Loss D: 0.009854251518845558, Loss G: 1.146228313446045\n",
            "Epoch [4300/10000]  Loss D: 0.05445702373981476, Loss G: 1.4509620666503906\n",
            "Epoch [4400/10000]  Loss D: 0.0461924746632576, Loss G: 0.6999709010124207\n",
            "Epoch [4500/10000]  Loss D: 0.008814224973320961, Loss G: 1.1625357866287231\n",
            "Epoch [4600/10000]  Loss D: 0.010672969743609428, Loss G: 0.7351111769676208\n",
            "Epoch [4700/10000]  Loss D: 0.023442236706614494, Loss G: 1.1732245683670044\n",
            "Epoch [4800/10000]  Loss D: 0.015566287562251091, Loss G: 1.0020636320114136\n",
            "Epoch [4900/10000]  Loss D: 0.02452140860259533, Loss G: 1.1717036962509155\n",
            "Epoch [5000/10000]  Loss D: 0.0010488801635801792, Loss G: 0.9570121765136719\n",
            "Epoch [5100/10000]  Loss D: 0.04307381808757782, Loss G: 1.4434419870376587\n",
            "Epoch [5200/10000]  Loss D: 0.007973349653184414, Loss G: 1.1122055053710938\n",
            "Epoch [5300/10000]  Loss D: 0.03995378687977791, Loss G: 1.542930245399475\n",
            "Epoch [5400/10000]  Loss D: 0.043224141001701355, Loss G: 1.1658748388290405\n",
            "Epoch [5500/10000]  Loss D: 0.01149095967411995, Loss G: 0.810716450214386\n",
            "Epoch [5600/10000]  Loss D: 0.043756335973739624, Loss G: 1.1949349641799927\n",
            "Epoch [5700/10000]  Loss D: 0.006409564055502415, Loss G: 0.8918080925941467\n",
            "Epoch [5800/10000]  Loss D: 0.003545727813616395, Loss G: 0.904589831829071\n",
            "Epoch [5900/10000]  Loss D: 0.038649782538414, Loss G: 0.7129747271537781\n",
            "Epoch [6000/10000]  Loss D: 0.0009019607678055763, Loss G: 1.058720350265503\n",
            "Epoch [6100/10000]  Loss D: 1.8865996025851928e-05, Loss G: 0.9982855319976807\n",
            "Epoch [6200/10000]  Loss D: 0.013661835342645645, Loss G: 0.7338618040084839\n",
            "Epoch [6300/10000]  Loss D: 0.004100462421774864, Loss G: 1.14383864402771\n",
            "Epoch [6400/10000]  Loss D: 0.09360060095787048, Loss G: 0.9498468041419983\n",
            "Epoch [6500/10000]  Loss D: 0.0339374914765358, Loss G: 1.2775393724441528\n",
            "Epoch [6600/10000]  Loss D: 0.007060583680868149, Loss G: 1.2384170293807983\n",
            "Epoch [6700/10000]  Loss D: 0.042306434363126755, Loss G: 1.2660523653030396\n",
            "Epoch [6800/10000]  Loss D: 0.010707924142479897, Loss G: 0.8743643164634705\n",
            "Epoch [6900/10000]  Loss D: 0.018165702000260353, Loss G: 1.0949174165725708\n",
            "Epoch [7000/10000]  Loss D: 0.04898931086063385, Loss G: 1.531671404838562\n",
            "Epoch [7100/10000]  Loss D: 0.011762703768908978, Loss G: 0.732007622718811\n",
            "Epoch [7200/10000]  Loss D: 0.004258383065462112, Loss G: 1.0604643821716309\n",
            "Epoch [7300/10000]  Loss D: 0.026989426463842392, Loss G: 1.236413836479187\n",
            "Epoch [7400/10000]  Loss D: 0.003670240519568324, Loss G: 1.1224727630615234\n",
            "Epoch [7500/10000]  Loss D: 0.10835627466440201, Loss G: 0.430205762386322\n",
            "Epoch [7600/10000]  Loss D: 0.001464915694668889, Loss G: 0.9109051823616028\n",
            "Epoch [7700/10000]  Loss D: 0.026880880817770958, Loss G: 1.3747224807739258\n",
            "Epoch [7800/10000]  Loss D: 0.027783462777733803, Loss G: 0.7333917617797852\n",
            "Epoch [7900/10000]  Loss D: 0.019709214568138123, Loss G: 0.7831594944000244\n",
            "Epoch [8000/10000]  Loss D: 0.027191372588276863, Loss G: 0.6818955540657043\n",
            "Epoch [8100/10000]  Loss D: 0.018501192331314087, Loss G: 1.230312466621399\n",
            "Epoch [8200/10000]  Loss D: 0.01493789441883564, Loss G: 0.8527212738990784\n",
            "Epoch [8300/10000]  Loss D: 0.025421392172574997, Loss G: 0.5622534155845642\n",
            "Epoch [8400/10000]  Loss D: 0.0031979046761989594, Loss G: 0.8747150301933289\n",
            "Epoch [8500/10000]  Loss D: 0.007550537120550871, Loss G: 0.9015061259269714\n",
            "Epoch [8600/10000]  Loss D: 0.0052205524407327175, Loss G: 1.1190431118011475\n",
            "Epoch [8700/10000]  Loss D: 0.6579937934875488, Loss G: 0.13863570988178253\n",
            "Epoch [8800/10000]  Loss D: 0.036356180906295776, Loss G: 0.4694385528564453\n",
            "Epoch [8900/10000]  Loss D: 0.003357791341841221, Loss G: 0.9992132782936096\n",
            "Epoch [9000/10000]  Loss D: 0.016477983444929123, Loss G: 1.094414234161377\n",
            "Epoch [9100/10000]  Loss D: 0.12762925028800964, Loss G: 1.0255489349365234\n",
            "Epoch [9200/10000]  Loss D: 0.3054555356502533, Loss G: 0.5399929285049438\n",
            "Epoch [9300/10000]  Loss D: 0.06024813652038574, Loss G: 0.9883620142936707\n",
            "Epoch [9400/10000]  Loss D: 0.6825889945030212, Loss G: 2.2319860458374023\n",
            "Epoch [9500/10000]  Loss D: 0.06345164775848389, Loss G: 1.0835270881652832\n",
            "Epoch [9600/10000]  Loss D: 0.024761522188782692, Loss G: 0.9429552555084229\n",
            "Epoch [9700/10000]  Loss D: 0.05238661542534828, Loss G: 1.0702226161956787\n",
            "Epoch [9800/10000]  Loss D: 0.25925007462501526, Loss G: 0.3771924376487732\n",
            "Epoch [9900/10000]  Loss D: 0.026966650038957596, Loss G: 0.9913383722305298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.3 WGAN with Weight Clipping and Gradient Penalty"
      ],
      "metadata": {
        "id": "ixsja7t7mP9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision matplotlib Pillow numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPuVKLOKmKiR",
        "outputId": "cb993d0e-cf74-47b0-b4cd-5f0c1f2a4148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "LATENT_DIM = 100\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 0.00005\n",
        "CLIPPING_VALUE = 0.01\n",
        "CRITIC_ITERATIONS = 5\n",
        "IMAGE_SIZE = 28 * 28\n",
        "\n",
        "# Device configuration (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Generator model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.fc3 = nn.Linear(512, 1024)\n",
        "        self.fc4 = nn.Linear(1024, IMAGE_SIZE)\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = torch.relu(self.fc1(z))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        return torch.tanh(self.fc4(x)).view(-1, 1, 28, 28)  # Reshape to 28x28 image\n",
        "\n",
        "# Critic (Discriminator) model\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(IMAGE_SIZE, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "        self.lrelu = nn.LeakyReLU(0.2)  # Define LeakyReLU as a layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrelu(self.fc1(x))  # Use LeakyReLU layer\n",
        "        x = self.lrelu(self.fc2(x))  # Use LeakyReLU layer\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "# WGAN with weight clipping\n",
        "def train_wgan_weight_clipping(train_loader, generator, critic, epochs, critic_iterations, device):\n",
        "    opt_gen = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "    opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        for i, (real_images, _) in enumerate(train_loader):\n",
        "            real_images = real_images.view(-1, IMAGE_SIZE).to(device)\n",
        "\n",
        "            # Train Critic\n",
        "            for _ in range(CRITIC_ITERATIONS):\n",
        "                # Generate fake images\n",
        "                noise = torch.randn(BATCH_SIZE, LATENT_DIM).to(device)\n",
        "                fake_images = generator(noise).view(BATCH_SIZE, -1)\n",
        "\n",
        "                # Critic loss\n",
        "                real_score = critic(real_images)\n",
        "                fake_score = critic(fake_images)\n",
        "                critic_loss = fake_score.mean() - real_score.mean()\n",
        "\n",
        "                opt_critic.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                opt_critic.step()\n",
        "\n",
        "                # Clip weights\n",
        "                for p in critic.parameters():\n",
        "                    p.data.clamp_(-CLIPPING_VALUE, CLIPPING_VALUE)\n",
        "\n",
        "            # Train Generator\n",
        "            noise = torch.randn(BATCH_SIZE, LATENT_DIM).to(device)\n",
        "            fake_images = generator(noise).view(BATCH_SIZE, -1)\n",
        "            fake_score = critic(fake_images)\n",
        "\n",
        "            generator_loss = -fake_score.mean()\n",
        "\n",
        "            opt_gen.zero_grad()\n",
        "            generator_loss.backward()\n",
        "            opt_gen.step()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f'Epoch [{epoch}/{EPOCHS}], Batch [{i}/{len(train_loader)}]')\n",
        "\n",
        "        # Save generated images after each epoch\n",
        "        save_generated_images(generator, epoch)\n",
        "\n",
        "# WGAN with gradient penalty\n",
        "def train_wgan_gradient_penalty(train_loader, generator, critic, epochs, critic_iterations, device, lambda_gp=10.0):\n",
        "    opt_gen = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "    opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        for i, (real_images, _) in enumerate(train_loader):\n",
        "            real_images = real_images.view(-1, IMAGE_SIZE).to(device)\n",
        "\n",
        "            # Train Critic\n",
        "            for _ in range(CRITIC_ITERATIONS):\n",
        "                # Generate fake images\n",
        "                noise = torch.randn(BATCH_SIZE, LATENT_DIM).to(device)\n",
        "                fake_images = generator(noise).view(BATCH_SIZE, -1)\n",
        "\n",
        "                # Critic loss\n",
        "                real_score = critic(real_images)\n",
        "                fake_score = critic(fake_images)\n",
        "\n",
        "                # Compute gradient penalty\n",
        "                epsilon = torch.rand(BATCH_SIZE, 1).to(device)\n",
        "                interpolated_images = epsilon * real_images + (1 - epsilon) * fake_images\n",
        "                interpolated_images.requires_grad_(True)\n",
        "\n",
        "                interpolated_score = critic(interpolated_images)\n",
        "                gradients = torch.autograd.grad(outputs=interpolated_score, inputs=interpolated_images,\n",
        "                                                grad_outputs=torch.ones_like(interpolated_score), create_graph=True)[0]\n",
        "                gradients = gradients.view(BATCH_SIZE, -1)\n",
        "                gradient_norm = gradients.norm(2, dim=1)\n",
        "                gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
        "\n",
        "                critic_loss = fake_score.mean() - real_score.mean() + lambda_gp * gradient_penalty\n",
        "\n",
        "                opt_critic.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                opt_critic.step()\n",
        "\n",
        "            # Train Generator\n",
        "            noise = torch.randn(BATCH_SIZE, LATENT_DIM).to(device)\n",
        "            fake_images = generator(noise).view(BATCH_SIZE, -1)\n",
        "            fake_score = critic(fake_images)\n",
        "\n",
        "            generator_loss = -fake_score.mean()\n",
        "\n",
        "            opt_gen.zero_grad()\n",
        "            generator_loss.backward()\n",
        "            opt_gen.step()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f'Epoch [{epoch}/{EPOCHS}], Batch [{i}/{len(train_loader)}]')\n",
        "\n",
        "        # Save generated images after each epoch\n",
        "        save_generated_images(generator, epoch)\n",
        "\n",
        "# Helper function to save generated images\n",
        "def save_generated_images(generator, epoch):\n",
        "    noise = torch.randn(64, LATENT_DIM).to(device)\n",
        "    fake_images = generator(noise).view(64, 1, 28, 28).cpu().detach()\n",
        "\n",
        "    # Save images\n",
        "    output_dir = \"output_wgan\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for i in range(64):\n",
        "        img = fake_images[i].squeeze().numpy()\n",
        "        img = (img * 255.0).astype(np.uint8)\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(f\"{output_dir}/epoch{epoch}_img{i}.png\")\n",
        "\n",
        "# Main program\n",
        "if __name__ == '__main__':\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "    dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Initialize models\n",
        "    generator = Generator(LATENT_DIM).to(device)\n",
        "    critic = Critic().to(device)\n",
        "\n",
        "    # Train WGAN with weight clipping\n",
        "    print(\"Training WGAN with Weight Clipping...\")\n",
        "    train_wgan_weight_clipping(train_loader, generator, critic, EPOCHS, CRITIC_ITERATIONS, device)\n",
        "\n",
        "    # Train WGAN with gradient penalty\n",
        "    print(\"Training WGAN with Gradient Penalty...\")\n",
        "    train_wgan_gradient_penalty(train_loader, generator, critic, EPOCHS, CRITIC_ITERATIONS, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-pBRNScCmRnO",
        "outputId": "6d6ee0b0-969e-488b-932e-daa133f19e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training WGAN with Weight Clipping...\n",
            "Epoch [0/5], Batch [0/938]\n",
            "Epoch [0/5], Batch [100/938]\n",
            "Epoch [0/5], Batch [200/938]\n",
            "Epoch [0/5], Batch [300/938]\n",
            "Epoch [0/5], Batch [400/938]\n",
            "Epoch [0/5], Batch [500/938]\n",
            "Epoch [0/5], Batch [600/938]\n",
            "Epoch [0/5], Batch [700/938]\n",
            "Epoch [0/5], Batch [800/938]\n",
            "Epoch [0/5], Batch [900/938]\n",
            "Epoch [1/5], Batch [0/938]\n",
            "Epoch [1/5], Batch [100/938]\n",
            "Epoch [1/5], Batch [200/938]\n",
            "Epoch [1/5], Batch [300/938]\n",
            "Epoch [1/5], Batch [400/938]\n",
            "Epoch [1/5], Batch [500/938]\n",
            "Epoch [1/5], Batch [600/938]\n",
            "Epoch [1/5], Batch [700/938]\n",
            "Epoch [1/5], Batch [800/938]\n",
            "Epoch [1/5], Batch [900/938]\n",
            "Epoch [2/5], Batch [0/938]\n",
            "Epoch [2/5], Batch [100/938]\n",
            "Epoch [2/5], Batch [200/938]\n",
            "Epoch [2/5], Batch [300/938]\n",
            "Epoch [2/5], Batch [400/938]\n",
            "Epoch [2/5], Batch [500/938]\n",
            "Epoch [2/5], Batch [600/938]\n",
            "Epoch [2/5], Batch [700/938]\n",
            "Epoch [2/5], Batch [800/938]\n",
            "Epoch [2/5], Batch [900/938]\n",
            "Epoch [3/5], Batch [0/938]\n",
            "Epoch [3/5], Batch [100/938]\n",
            "Epoch [3/5], Batch [200/938]\n",
            "Epoch [3/5], Batch [300/938]\n",
            "Epoch [3/5], Batch [400/938]\n",
            "Epoch [3/5], Batch [500/938]\n",
            "Epoch [3/5], Batch [600/938]\n",
            "Epoch [3/5], Batch [700/938]\n",
            "Epoch [3/5], Batch [800/938]\n",
            "Epoch [3/5], Batch [900/938]\n",
            "Epoch [4/5], Batch [0/938]\n",
            "Epoch [4/5], Batch [100/938]\n",
            "Epoch [4/5], Batch [200/938]\n",
            "Epoch [4/5], Batch [300/938]\n",
            "Epoch [4/5], Batch [400/938]\n",
            "Epoch [4/5], Batch [500/938]\n",
            "Epoch [4/5], Batch [600/938]\n",
            "Epoch [4/5], Batch [700/938]\n",
            "Epoch [4/5], Batch [800/938]\n",
            "Epoch [4/5], Batch [900/938]\n",
            "Training WGAN with Gradient Penalty...\n",
            "Epoch [0/5], Batch [0/938]\n",
            "Epoch [0/5], Batch [100/938]\n",
            "Epoch [0/5], Batch [200/938]\n",
            "Epoch [0/5], Batch [300/938]\n",
            "Epoch [0/5], Batch [400/938]\n",
            "Epoch [0/5], Batch [500/938]\n",
            "Epoch [0/5], Batch [600/938]\n",
            "Epoch [0/5], Batch [700/938]\n",
            "Epoch [0/5], Batch [800/938]\n",
            "Epoch [0/5], Batch [900/938]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2226f9d24780>\u001b[0m in \u001b[0;36m<cell line: 167>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;31m# Train WGAN with gradient penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training WGAN with Gradient Penalty...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mtrain_wgan_gradient_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCRITIC_ITERATIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-2226f9d24780>\u001b[0m in \u001b[0;36mtrain_wgan_gradient_penalty\u001b[0;34m(train_loader, generator, critic, epochs, critic_iterations, device, lambda_gp)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;31m# Compute gradient penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0minterpolated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreal_images\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfake_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0minterpolated_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Hyperparameters\n",
        "LATENT_DIM = 100\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 0.00005\n",
        "CLIPPING_VALUE = 0.01\n",
        "CRITIC_ITERATIONS = 5\n",
        "IMAGE_SIZE = 28 * 28\n",
        "\n",
        "# Device configuration (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Generator model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.fc3 = nn.Linear(512, 1024)\n",
        "        self.fc4 = nn.Linear(1024, IMAGE_SIZE)\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = torch.relu(self.fc1(z))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        return torch.tanh(self.fc4(x)).view(-1, 1, 28, 28)  # Reshape to 28x28 image\n",
        "\n",
        "# Critic (Discriminator) model\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(IMAGE_SIZE, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "        self.lrelu = nn.LeakyReLU(0.2)  # Define LeakyReLU as a layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrelu(self.fc1(x))  # Use LeakyReLU layer\n",
        "        x = self.lrelu(self.fc2(x))  # Use LeakyReLU layer\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "# WGAN with weight clipping\n",
        "def train_wgan_weight_clipping(train_loader, generator, critic, epochs, critic_iterations, device):\n",
        "    opt_gen = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "    opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        for i, (real_images, _) in enumerate(train_loader):\n",
        "            real_images = real_images.view(-1, IMAGE_SIZE).to(device)\n",
        "\n",
        "            # Train Critic\n",
        "            for _ in range(CRITIC_ITERATIONS):\n",
        "                # Generate fake images\n",
        "                noise = torch.randn(BATCH_SIZE, LATENT_DIM).to(device)\n",
        "                fake_images = generator(noise).view(BATCH_SIZE, -1)\n",
        "\n",
        "                # Critic loss\n",
        "                real_score = critic(real_images)\n",
        "                fake_score = critic(fake_images)\n",
        "                critic_loss = fake_score.mean() - real_score.mean()\n",
        "\n",
        "                opt_critic.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                opt_critic.step()\n",
        "\n",
        "                # Clip weights\n",
        "                for p in critic.parameters():\n",
        "                    p.data.clamp_(-CLIPPING_VALUE, CLIPPING_VALUE)\n",
        "\n",
        "            # Train Generator\n",
        "            noise = torch.randn(BATCH_SIZE, LATENT_DIM).to(device)\n",
        "            fake_images = generator(noise).view(BATCH_SIZE, -1)\n",
        "            fake_score = critic(fake_images)\n",
        "\n",
        "            generator_loss = -fake_score.mean()\n",
        "\n",
        "            opt_gen.zero_grad()\n",
        "            generator_loss.backward()\n",
        "            opt_gen.step()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f'Epoch [{epoch}/{EPOCHS}], Batch [{i}/{len(train_loader)}]')\n",
        "\n",
        "        # Save generated images after each epoch\n",
        "        save_generated_images(generator, epoch)\n",
        "\n",
        "# WGAN with gradient penalty\n",
        "def train_wgan_gradient_penalty(train_loader, generator, critic, epochs, critic_iterations, device, lambda_gp=10.0):\n",
        "    opt_gen = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "    opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        for i, (real_images, _) in enumerate(train_loader):\n",
        "            real_images = real_images.view(-1, IMAGE_SIZE).to(device)\n",
        "\n",
        "            # Train Critic\n",
        "            for _ in range(CRITIC_ITERATIONS):\n",
        "                # Generate fake images\n",
        "                noise = torch.randn(real_images.size(0), LATENT_DIM).to(device)  # Match batch size of real_images\n",
        "                fake_images = generator(noise).view(real_images.size(0), -1)\n",
        "\n",
        "                # Critic loss\n",
        "                real_score = critic(real_images)\n",
        "                fake_score = critic(fake_images)\n",
        "\n",
        "                # Compute gradient penalty\n",
        "                epsilon = torch.rand(real_images.size(0), 1).to(device)  # Use the same batch size for epsilon\n",
        "                interpolated_images = epsilon * real_images + (1 - epsilon) * fake_images\n",
        "                interpolated_images.requires_grad_(True)\n",
        "\n",
        "                interpolated_score = critic(interpolated_images)\n",
        "                gradients = torch.autograd.grad(outputs=interpolated_score, inputs=interpolated_images,\n",
        "                                                grad_outputs=torch.ones_like(interpolated_score), create_graph=True)[0]\n",
        "                gradients = gradients.view(real_images.size(0), -1)\n",
        "                gradient_norm = gradients.norm(2, dim=1)\n",
        "                gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
        "\n",
        "                critic_loss = fake_score.mean() - real_score.mean() + lambda_gp * gradient_penalty\n",
        "\n",
        "                opt_critic.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                opt_critic.step()\n",
        "\n",
        "            # Train Generator\n",
        "            noise = torch.randn(real_images.size(0), LATENT_DIM).to(device)  # Match batch size of real_images\n",
        "            fake_images = generator(noise).view(real_images.size(0), -1)\n",
        "            fake_score = critic(fake_images)\n",
        "\n",
        "            generator_loss = -fake_score.mean()\n",
        "\n",
        "            opt_gen.zero_grad()\n",
        "            generator_loss.backward()\n",
        "            opt_gen.step()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f'Epoch [{epoch}/{EPOCHS}], Batch [{i}/{len(train_loader)}]')\n",
        "\n",
        "        # Save generated images after each epoch\n",
        "        save_generated_images(generator, epoch)\n",
        "\n",
        "\n",
        "# Helper function to save generated images\n",
        "def save_generated_images(generator, epoch):\n",
        "    noise = torch.randn(64, LATENT_DIM).to(device)\n",
        "    fake_images = generator(noise).view(64, 1, 28, 28).cpu().detach()\n",
        "\n",
        "    # Save images\n",
        "    output_dir = \"output_wgan\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for i in range(64):\n",
        "        img = fake_images[i].squeeze().numpy()\n",
        "        img = (img * 255.0).astype(np.uint8)\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(f\"{output_dir}/epoch{epoch}_img{i}.png\")\n",
        "\n",
        "# Main program\n",
        "if __name__ == '__main__':\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "    dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Initialize models\n",
        "    generator = Generator(LATENT_DIM).to(device)\n",
        "    critic = Critic().to(device)\n",
        "\n",
        "    # Train WGAN with weight clipping\n",
        "    print(\"Training WGAN with Weight Clipping...\")\n",
        "    train_wgan_weight_clipping(train_loader, generator, critic, EPOCHS, CRITIC_ITERATIONS, device)\n",
        "\n",
        "    # Train WGAN with gradient penalty\n",
        "    print(\"Training WGAN with Gradient Penalty...\")\n",
        "    train_wgan_gradient_penalty(train_loader, generator, critic, EPOCHS, CRITIC_ITERATIONS, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nKsljnknp_v",
        "outputId": "e47a5842-9cfd-4548-8063-09b1f56de88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training WGAN with Weight Clipping...\n",
            "Epoch [0/5], Batch [0/938]\n",
            "Epoch [0/5], Batch [100/938]\n",
            "Epoch [0/5], Batch [200/938]\n",
            "Epoch [0/5], Batch [300/938]\n",
            "Epoch [0/5], Batch [400/938]\n",
            "Epoch [0/5], Batch [500/938]\n",
            "Epoch [0/5], Batch [600/938]\n",
            "Epoch [0/5], Batch [700/938]\n",
            "Epoch [0/5], Batch [800/938]\n",
            "Epoch [0/5], Batch [900/938]\n",
            "Epoch [1/5], Batch [0/938]\n",
            "Epoch [1/5], Batch [100/938]\n",
            "Epoch [1/5], Batch [200/938]\n",
            "Epoch [1/5], Batch [300/938]\n",
            "Epoch [1/5], Batch [400/938]\n",
            "Epoch [1/5], Batch [500/938]\n",
            "Epoch [1/5], Batch [600/938]\n",
            "Epoch [1/5], Batch [700/938]\n",
            "Epoch [1/5], Batch [800/938]\n",
            "Epoch [1/5], Batch [900/938]\n",
            "Epoch [2/5], Batch [0/938]\n",
            "Epoch [2/5], Batch [100/938]\n",
            "Epoch [2/5], Batch [200/938]\n",
            "Epoch [2/5], Batch [300/938]\n",
            "Epoch [2/5], Batch [400/938]\n",
            "Epoch [2/5], Batch [500/938]\n",
            "Epoch [2/5], Batch [600/938]\n",
            "Epoch [2/5], Batch [700/938]\n",
            "Epoch [2/5], Batch [800/938]\n",
            "Epoch [2/5], Batch [900/938]\n",
            "Epoch [3/5], Batch [0/938]\n",
            "Epoch [3/5], Batch [100/938]\n",
            "Epoch [3/5], Batch [200/938]\n",
            "Epoch [3/5], Batch [300/938]\n",
            "Epoch [3/5], Batch [400/938]\n",
            "Epoch [3/5], Batch [500/938]\n",
            "Epoch [3/5], Batch [600/938]\n",
            "Epoch [3/5], Batch [700/938]\n",
            "Epoch [3/5], Batch [800/938]\n",
            "Epoch [3/5], Batch [900/938]\n",
            "Epoch [4/5], Batch [0/938]\n",
            "Epoch [4/5], Batch [100/938]\n",
            "Epoch [4/5], Batch [200/938]\n",
            "Epoch [4/5], Batch [300/938]\n",
            "Epoch [4/5], Batch [400/938]\n",
            "Epoch [4/5], Batch [500/938]\n",
            "Epoch [4/5], Batch [600/938]\n",
            "Epoch [4/5], Batch [700/938]\n",
            "Epoch [4/5], Batch [800/938]\n",
            "Epoch [4/5], Batch [900/938]\n",
            "Training WGAN with Gradient Penalty...\n",
            "Epoch [0/5], Batch [0/938]\n",
            "Epoch [0/5], Batch [100/938]\n",
            "Epoch [0/5], Batch [200/938]\n",
            "Epoch [0/5], Batch [300/938]\n",
            "Epoch [0/5], Batch [400/938]\n",
            "Epoch [0/5], Batch [500/938]\n",
            "Epoch [0/5], Batch [600/938]\n",
            "Epoch [0/5], Batch [700/938]\n",
            "Epoch [0/5], Batch [800/938]\n",
            "Epoch [0/5], Batch [900/938]\n",
            "Epoch [1/5], Batch [0/938]\n",
            "Epoch [1/5], Batch [100/938]\n",
            "Epoch [1/5], Batch [200/938]\n",
            "Epoch [1/5], Batch [300/938]\n",
            "Epoch [1/5], Batch [400/938]\n",
            "Epoch [1/5], Batch [500/938]\n",
            "Epoch [1/5], Batch [600/938]\n",
            "Epoch [1/5], Batch [700/938]\n",
            "Epoch [1/5], Batch [800/938]\n",
            "Epoch [1/5], Batch [900/938]\n",
            "Epoch [2/5], Batch [0/938]\n",
            "Epoch [2/5], Batch [100/938]\n",
            "Epoch [2/5], Batch [200/938]\n",
            "Epoch [2/5], Batch [300/938]\n",
            "Epoch [2/5], Batch [400/938]\n",
            "Epoch [2/5], Batch [500/938]\n",
            "Epoch [2/5], Batch [600/938]\n",
            "Epoch [2/5], Batch [700/938]\n",
            "Epoch [2/5], Batch [800/938]\n",
            "Epoch [2/5], Batch [900/938]\n",
            "Epoch [3/5], Batch [0/938]\n",
            "Epoch [3/5], Batch [100/938]\n",
            "Epoch [3/5], Batch [200/938]\n",
            "Epoch [3/5], Batch [300/938]\n",
            "Epoch [3/5], Batch [400/938]\n",
            "Epoch [3/5], Batch [500/938]\n",
            "Epoch [3/5], Batch [600/938]\n",
            "Epoch [3/5], Batch [700/938]\n",
            "Epoch [3/5], Batch [800/938]\n",
            "Epoch [3/5], Batch [900/938]\n",
            "Epoch [4/5], Batch [0/938]\n",
            "Epoch [4/5], Batch [100/938]\n",
            "Epoch [4/5], Batch [200/938]\n",
            "Epoch [4/5], Batch [300/938]\n",
            "Epoch [4/5], Batch [400/938]\n",
            "Epoch [4/5], Batch [500/938]\n",
            "Epoch [4/5], Batch [600/938]\n",
            "Epoch [4/5], Batch [700/938]\n",
            "Epoch [4/5], Batch [800/938]\n",
            "Epoch [4/5], Batch [900/938]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.4"
      ],
      "metadata": {
        "id": "PYwmsPfhnLsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Custom dataset for the uploaded images\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            return self.transform(image)\n",
        "        return image\n",
        "\n",
        "# Generator network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 3 * 64 * 64),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z).view(-1, 3, 64, 64)\n",
        "\n",
        "# Discriminator network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3 * 64 * 64, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x.view(-1, 3 * 64 * 64))\n",
        "\n",
        "# Hyperparameters\n",
        "z_dim = 100\n",
        "lr = 0.0002\n",
        "batch_size = 1\n",
        "epochs = 100\n",
        "\n",
        "# Transformations and dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
        "])\n",
        "image_paths = list(uploaded.keys())\n",
        "dataset = CustomImageDataset(image_paths, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator(z_dim)\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "g_optim = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "d_optim = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for real_images in dataloader:\n",
        "        # Train Discriminator\n",
        "        real_labels = torch.ones(batch_size, 1)\n",
        "        fake_labels = torch.zeros(batch_size, 1)\n",
        "\n",
        "        # Real images\n",
        "        outputs = discriminator(real_images)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "        real_score = outputs\n",
        "\n",
        "        # Fake images\n",
        "        z = torch.randn(batch_size, z_dim)\n",
        "        fake_images = generator(z)\n",
        "        outputs = discriminator(fake_images.detach())\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "        fake_score = outputs\n",
        "\n",
        "        # Backprop and optimize\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_optim.zero_grad()\n",
        "        d_loss.backward()\n",
        "        d_optim.step()\n",
        "\n",
        "        # Train Generator\n",
        "        z = torch.randn(batch_size, z_dim)\n",
        "        fake_images = generator(z)\n",
        "        outputs = discriminator(fake_images)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        g_optim.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optim.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n",
        "\n",
        "# Save generated image\n",
        "z = torch.randn(1, z_dim)\n",
        "generated_image = generator(z).detach().squeeze()\n",
        "generated_image = (generated_image + 1) / 2  # Denormalize\n",
        "generated_image = transforms.ToPILImage()(generated_image)\n",
        "generated_image.save('generated_image.jpg')\n"
      ],
      "metadata": {
        "id": "WQmR5bA9cAvK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3921d3ec-66d1-46de-8052-c3909ebf51ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a1d7c651-ae5b-49a5-bab1-2aab92dfd317\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a1d7c651-ae5b-49a5-bab1-2aab92dfd317\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving file.jpg to file.jpg\n",
            "Epoch [1/100], d_loss: 1.3718, g_loss: 0.7066\n",
            "Epoch [2/100], d_loss: 0.8613, g_loss: 0.6973\n",
            "Epoch [3/100], d_loss: 0.7221, g_loss: 0.6917\n",
            "Epoch [4/100], d_loss: 0.7353, g_loss: 0.6767\n",
            "Epoch [5/100], d_loss: 0.7573, g_loss: 0.6715\n",
            "Epoch [6/100], d_loss: 0.7782, g_loss: 0.6957\n",
            "Epoch [7/100], d_loss: 0.7628, g_loss: 0.7463\n",
            "Epoch [8/100], d_loss: 0.7124, g_loss: 0.7806\n",
            "Epoch [9/100], d_loss: 0.6929, g_loss: 0.7746\n",
            "Epoch [10/100], d_loss: 0.7099, g_loss: 0.8039\n",
            "Epoch [11/100], d_loss: 0.7858, g_loss: 0.6835\n",
            "Epoch [12/100], d_loss: 0.9849, g_loss: 0.7220\n",
            "Epoch [13/100], d_loss: 0.8941, g_loss: 0.5623\n",
            "Epoch [14/100], d_loss: 1.1052, g_loss: 0.6208\n",
            "Epoch [15/100], d_loss: 0.9371, g_loss: 0.6492\n",
            "Epoch [16/100], d_loss: 1.0009, g_loss: 0.6734\n",
            "Epoch [17/100], d_loss: 0.8882, g_loss: 0.7538\n",
            "Epoch [18/100], d_loss: 0.7855, g_loss: 0.8192\n",
            "Epoch [19/100], d_loss: 0.7562, g_loss: 0.9751\n",
            "Epoch [20/100], d_loss: 0.7755, g_loss: 0.7074\n",
            "Epoch [21/100], d_loss: 1.1131, g_loss: 0.9069\n",
            "Epoch [22/100], d_loss: 0.8993, g_loss: 0.7161\n",
            "Epoch [23/100], d_loss: 0.9074, g_loss: 0.8218\n",
            "Epoch [24/100], d_loss: 0.9040, g_loss: 0.5248\n",
            "Epoch [25/100], d_loss: 1.0309, g_loss: 0.7494\n",
            "Epoch [26/100], d_loss: 0.7957, g_loss: 0.9774\n",
            "Epoch [27/100], d_loss: 0.9304, g_loss: 0.3925\n",
            "Epoch [28/100], d_loss: 1.5564, g_loss: 0.4447\n",
            "Epoch [29/100], d_loss: 1.2437, g_loss: 0.7181\n",
            "Epoch [30/100], d_loss: 0.8299, g_loss: 1.1191\n",
            "Epoch [31/100], d_loss: 1.3777, g_loss: 0.4957\n",
            "Epoch [32/100], d_loss: 1.2789, g_loss: 0.4881\n",
            "Epoch [33/100], d_loss: 1.2150, g_loss: 0.5475\n",
            "Epoch [34/100], d_loss: 0.9898, g_loss: 0.6747\n",
            "Epoch [35/100], d_loss: 1.0894, g_loss: 0.5351\n",
            "Epoch [36/100], d_loss: 1.1872, g_loss: 0.6258\n",
            "Epoch [37/100], d_loss: 1.0806, g_loss: 0.5810\n",
            "Epoch [38/100], d_loss: 1.1472, g_loss: 0.7439\n",
            "Epoch [39/100], d_loss: 1.1414, g_loss: 0.5178\n",
            "Epoch [40/100], d_loss: 1.0935, g_loss: 0.7229\n",
            "Epoch [41/100], d_loss: 1.0189, g_loss: 1.0041\n",
            "Epoch [42/100], d_loss: 1.2082, g_loss: 0.2917\n",
            "Epoch [43/100], d_loss: 1.6855, g_loss: 0.3869\n",
            "Epoch [44/100], d_loss: 1.2789, g_loss: 0.5984\n",
            "Epoch [45/100], d_loss: 1.2335, g_loss: 0.6752\n",
            "Epoch [46/100], d_loss: 1.2724, g_loss: 0.5315\n",
            "Epoch [47/100], d_loss: 1.2183, g_loss: 0.4870\n",
            "Epoch [48/100], d_loss: 1.2235, g_loss: 0.5480\n",
            "Epoch [49/100], d_loss: 1.2343, g_loss: 0.6933\n",
            "Epoch [50/100], d_loss: 1.2293, g_loss: 0.6015\n",
            "Epoch [51/100], d_loss: 1.2508, g_loss: 0.8054\n",
            "Epoch [52/100], d_loss: 1.2562, g_loss: 0.5115\n",
            "Epoch [53/100], d_loss: 1.3451, g_loss: 0.7678\n",
            "Epoch [54/100], d_loss: 1.4033, g_loss: 0.4316\n",
            "Epoch [55/100], d_loss: 1.3597, g_loss: 0.4503\n",
            "Epoch [56/100], d_loss: 1.5834, g_loss: 0.7934\n",
            "Epoch [57/100], d_loss: 1.4360, g_loss: 0.6852\n",
            "Epoch [58/100], d_loss: 1.2464, g_loss: 0.5023\n",
            "Epoch [59/100], d_loss: 1.2305, g_loss: 0.8201\n",
            "Epoch [60/100], d_loss: 1.2075, g_loss: 0.6340\n",
            "Epoch [61/100], d_loss: 1.3507, g_loss: 0.9925\n",
            "Epoch [62/100], d_loss: 1.5524, g_loss: 0.3631\n",
            "Epoch [63/100], d_loss: 1.4276, g_loss: 0.3268\n",
            "Epoch [64/100], d_loss: 1.6125, g_loss: 0.7493\n",
            "Epoch [65/100], d_loss: 1.4084, g_loss: 0.8462\n",
            "Epoch [66/100], d_loss: 1.3313, g_loss: 0.6555\n",
            "Epoch [67/100], d_loss: 1.2172, g_loss: 0.8519\n",
            "Epoch [68/100], d_loss: 1.1292, g_loss: 0.8900\n",
            "Epoch [69/100], d_loss: 1.1245, g_loss: 0.6271\n",
            "Epoch [70/100], d_loss: 1.2851, g_loss: 1.2106\n",
            "Epoch [71/100], d_loss: 1.7936, g_loss: 0.1720\n",
            "Epoch [72/100], d_loss: 2.1766, g_loss: 0.3780\n",
            "Epoch [73/100], d_loss: 1.4696, g_loss: 1.0679\n",
            "Epoch [74/100], d_loss: 1.4572, g_loss: 0.9607\n",
            "Epoch [75/100], d_loss: 1.3204, g_loss: 0.6381\n",
            "Epoch [76/100], d_loss: 1.2857, g_loss: 0.6178\n",
            "Epoch [77/100], d_loss: 1.2693, g_loss: 0.5281\n",
            "Epoch [78/100], d_loss: 1.4155, g_loss: 0.9116\n",
            "Epoch [79/100], d_loss: 1.4947, g_loss: 0.5583\n",
            "Epoch [80/100], d_loss: 1.4525, g_loss: 0.4839\n",
            "Epoch [81/100], d_loss: 1.5097, g_loss: 0.7646\n",
            "Epoch [82/100], d_loss: 1.3861, g_loss: 0.7718\n",
            "Epoch [83/100], d_loss: 1.2953, g_loss: 0.7414\n",
            "Epoch [84/100], d_loss: 1.2150, g_loss: 0.8256\n",
            "Epoch [85/100], d_loss: 1.1852, g_loss: 0.7454\n",
            "Epoch [86/100], d_loss: 1.2686, g_loss: 1.3743\n",
            "Epoch [87/100], d_loss: 1.6793, g_loss: 0.2195\n",
            "Epoch [88/100], d_loss: 2.1028, g_loss: 0.5400\n",
            "Epoch [89/100], d_loss: 1.5062, g_loss: 0.8143\n",
            "Epoch [90/100], d_loss: 1.5424, g_loss: 0.6940\n",
            "Epoch [91/100], d_loss: 1.4216, g_loss: 0.5729\n",
            "Epoch [92/100], d_loss: 1.3713, g_loss: 0.7240\n",
            "Epoch [93/100], d_loss: 1.2555, g_loss: 0.8243\n",
            "Epoch [94/100], d_loss: 1.2031, g_loss: 0.7482\n",
            "Epoch [95/100], d_loss: 1.1775, g_loss: 1.1131\n",
            "Epoch [96/100], d_loss: 1.2688, g_loss: 0.5283\n",
            "Epoch [97/100], d_loss: 1.4634, g_loss: 0.8538\n",
            "Epoch [98/100], d_loss: 1.6215, g_loss: 0.3433\n",
            "Epoch [99/100], d_loss: 1.7222, g_loss: 0.7238\n",
            "Epoch [100/100], d_loss: 1.4207, g_loss: 0.7885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "# Custom dataset for the uploaded images\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            return self.transform(image)\n",
        "        return image\n",
        "\n",
        "# Function to display images\n",
        "def show_images(images, nrows=1, ncols=1):\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 3, nrows * 3))\n",
        "    for i, img in enumerate(images):\n",
        "        ax = axes[i] if nrows * ncols > 1 else axes\n",
        "        ax.imshow(img.permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Generator network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 3 * 64 * 64),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z).view(-1, 3, 64, 64)\n",
        "\n",
        "# Discriminator network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3 * 64 * 64, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x.view(-1, 3 * 64 * 64))\n",
        "\n",
        "# Hyperparameters\n",
        "z_dim = 100\n",
        "lr = 0.0002\n",
        "batch_size = 1\n",
        "epochs = 200\n",
        "\n",
        "# Transformations and dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
        "])\n",
        "image_paths = list(uploaded.keys())\n",
        "dataset = CustomImageDataset(image_paths, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator(z_dim)\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "g_optim = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "d_optim = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for real_images in dataloader:\n",
        "        # Train Discriminator\n",
        "        real_labels = torch.ones(batch_size, 1)\n",
        "        fake_labels = torch.zeros(batch_size, 1)\n",
        "\n",
        "        # Real images\n",
        "        outputs = discriminator(real_images)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "        real_score = outputs\n",
        "\n",
        "        # Fake images\n",
        "        z = torch.randn(batch_size, z_dim)\n",
        "        fake_images = generator(z)\n",
        "        outputs = discriminator(fake_images.detach())\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "        fake_score = outputs\n",
        "\n",
        "        # Backprop and optimize\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_optim.zero_grad()\n",
        "        d_loss.backward()\n",
        "        d_optim.step()\n",
        "\n",
        "        # Train Generator\n",
        "        z = torch.randn(batch_size, z_dim)\n",
        "        fake_images = generator(z)\n",
        "        outputs = discriminator(fake_images)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        g_optim.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optim.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n",
        "\n",
        "# Save and show generated image\n",
        "z = torch.randn(1, z_dim)\n",
        "generated_image = generator(z).detach().squeeze()\n",
        "generated_image = (generated_image + 1) / 2  # Denormalize\n",
        "show_images([generated_image])\n",
        "generated_image = transforms.ToPILImage()(generated_image)\n",
        "generated_image.save('generated_image.jpg')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kEqASBsce00O",
        "outputId": "e1e403fa-1891-4113-b77e-69c5782ae602"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0b33fcd9-3094-46d2-a223-4305a9cd2b6e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0b33fcd9-3094-46d2-a223-4305a9cd2b6e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving file.jpg to file (1).jpg\n",
            "Epoch [1/200], d_loss: 1.3987, g_loss: 0.7079\n",
            "Epoch [2/200], d_loss: 0.9531, g_loss: 0.6972\n",
            "Epoch [3/200], d_loss: 0.7341, g_loss: 0.6911\n",
            "Epoch [4/200], d_loss: 0.7401, g_loss: 0.6658\n",
            "Epoch [5/200], d_loss: 0.7529, g_loss: 0.6610\n",
            "Epoch [6/200], d_loss: 0.7840, g_loss: 0.6729\n",
            "Epoch [7/200], d_loss: 0.7654, g_loss: 0.6688\n",
            "Epoch [8/200], d_loss: 0.7578, g_loss: 0.7280\n",
            "Epoch [9/200], d_loss: 0.7406, g_loss: 0.7377\n",
            "Epoch [10/200], d_loss: 0.7483, g_loss: 0.7693\n",
            "Epoch [11/200], d_loss: 0.7613, g_loss: 0.6995\n",
            "Epoch [12/200], d_loss: 0.8699, g_loss: 0.7967\n",
            "Epoch [13/200], d_loss: 1.1402, g_loss: 0.4134\n",
            "Epoch [14/200], d_loss: 1.2651, g_loss: 0.3063\n",
            "Epoch [15/200], d_loss: 1.5661, g_loss: 0.3729\n",
            "Epoch [16/200], d_loss: 1.2376, g_loss: 0.5919\n",
            "Epoch [17/200], d_loss: 1.0306, g_loss: 0.7081\n",
            "Epoch [18/200], d_loss: 1.1045, g_loss: 0.6204\n",
            "Epoch [19/200], d_loss: 0.9691, g_loss: 0.6380\n",
            "Epoch [20/200], d_loss: 0.9365, g_loss: 0.7745\n",
            "Epoch [21/200], d_loss: 0.8924, g_loss: 0.6829\n",
            "Epoch [22/200], d_loss: 1.0078, g_loss: 0.9178\n",
            "Epoch [23/200], d_loss: 1.2227, g_loss: 0.2840\n",
            "Epoch [24/200], d_loss: 1.5939, g_loss: 0.3301\n",
            "Epoch [25/200], d_loss: 1.5146, g_loss: 0.5210\n",
            "Epoch [26/200], d_loss: 1.2257, g_loss: 0.6159\n",
            "Epoch [27/200], d_loss: 1.2757, g_loss: 0.5303\n",
            "Epoch [28/200], d_loss: 1.2379, g_loss: 0.5576\n",
            "Epoch [29/200], d_loss: 1.2043, g_loss: 0.8119\n",
            "Epoch [30/200], d_loss: 1.1353, g_loss: 0.5748\n",
            "Epoch [31/200], d_loss: 1.1690, g_loss: 1.0394\n",
            "Epoch [32/200], d_loss: 1.1439, g_loss: 0.5458\n",
            "Epoch [33/200], d_loss: 1.2348, g_loss: 0.8150\n",
            "Epoch [34/200], d_loss: 1.3454, g_loss: 0.5391\n",
            "Epoch [35/200], d_loss: 1.5003, g_loss: 0.5006\n",
            "Epoch [36/200], d_loss: 1.4629, g_loss: 0.5555\n",
            "Epoch [37/200], d_loss: 1.3721, g_loss: 0.6096\n",
            "Epoch [38/200], d_loss: 1.2205, g_loss: 0.8261\n",
            "Epoch [39/200], d_loss: 1.0494, g_loss: 0.6758\n",
            "Epoch [40/200], d_loss: 1.0475, g_loss: 1.9993\n",
            "Epoch [41/200], d_loss: 1.7119, g_loss: 0.1757\n",
            "Epoch [42/200], d_loss: 1.9434, g_loss: 0.3862\n",
            "Epoch [43/200], d_loss: 1.4646, g_loss: 0.7142\n",
            "Epoch [44/200], d_loss: 1.3900, g_loss: 0.6668\n",
            "Epoch [45/200], d_loss: 1.3578, g_loss: 0.5245\n",
            "Epoch [46/200], d_loss: 1.3543, g_loss: 0.6567\n",
            "Epoch [47/200], d_loss: 1.2752, g_loss: 0.6175\n",
            "Epoch [48/200], d_loss: 1.2727, g_loss: 0.9448\n",
            "Epoch [49/200], d_loss: 1.3300, g_loss: 0.3989\n",
            "Epoch [50/200], d_loss: 1.3597, g_loss: 0.6588\n",
            "Epoch [51/200], d_loss: 1.3344, g_loss: 0.7746\n",
            "Epoch [52/200], d_loss: 1.4198, g_loss: 0.4235\n",
            "Epoch [53/200], d_loss: 1.4625, g_loss: 0.5392\n",
            "Epoch [54/200], d_loss: 1.4174, g_loss: 0.6847\n",
            "Epoch [55/200], d_loss: 1.4156, g_loss: 0.5297\n",
            "Epoch [56/200], d_loss: 1.3641, g_loss: 0.5720\n",
            "Epoch [57/200], d_loss: 1.3000, g_loss: 0.6744\n",
            "Epoch [58/200], d_loss: 1.2628, g_loss: 0.5889\n",
            "Epoch [59/200], d_loss: 1.3141, g_loss: 0.9056\n",
            "Epoch [60/200], d_loss: 1.3647, g_loss: 0.5950\n",
            "Epoch [61/200], d_loss: 1.5142, g_loss: 0.9324\n",
            "Epoch [62/200], d_loss: 1.5797, g_loss: 0.4877\n",
            "Epoch [63/200], d_loss: 1.4978, g_loss: 0.6080\n",
            "Epoch [64/200], d_loss: 1.4347, g_loss: 0.7749\n",
            "Epoch [65/200], d_loss: 1.3533, g_loss: 0.6948\n",
            "Epoch [66/200], d_loss: 1.2253, g_loss: 0.7789\n",
            "Epoch [67/200], d_loss: 1.1399, g_loss: 1.4685\n",
            "Epoch [68/200], d_loss: 1.3199, g_loss: 0.2046\n",
            "Epoch [69/200], d_loss: 1.7487, g_loss: 0.7304\n",
            "Epoch [70/200], d_loss: 1.3070, g_loss: 0.9201\n",
            "Epoch [71/200], d_loss: 1.5273, g_loss: 0.4525\n",
            "Epoch [72/200], d_loss: 1.4325, g_loss: 0.3874\n",
            "Epoch [73/200], d_loss: 1.6084, g_loss: 0.6136\n",
            "Epoch [74/200], d_loss: 1.3941, g_loss: 0.8929\n",
            "Epoch [75/200], d_loss: 1.3953, g_loss: 0.7611\n",
            "Epoch [76/200], d_loss: 1.2985, g_loss: 0.7287\n",
            "Epoch [77/200], d_loss: 1.2639, g_loss: 0.7236\n",
            "Epoch [78/200], d_loss: 1.2686, g_loss: 0.7998\n",
            "Epoch [79/200], d_loss: 1.3128, g_loss: 0.5141\n",
            "Epoch [80/200], d_loss: 1.4628, g_loss: 0.9856\n",
            "Epoch [81/200], d_loss: 1.5914, g_loss: 0.3824\n",
            "Epoch [82/200], d_loss: 1.5919, g_loss: 0.6758\n",
            "Epoch [83/200], d_loss: 1.3521, g_loss: 0.7849\n",
            "Epoch [84/200], d_loss: 1.2773, g_loss: 0.7823\n",
            "Epoch [85/200], d_loss: 1.2382, g_loss: 1.0269\n",
            "Epoch [86/200], d_loss: 1.3469, g_loss: 0.3601\n",
            "Epoch [87/200], d_loss: 1.5848, g_loss: 0.7694\n",
            "Epoch [88/200], d_loss: 1.4648, g_loss: 0.8319\n",
            "Epoch [89/200], d_loss: 1.5545, g_loss: 0.5605\n",
            "Epoch [90/200], d_loss: 1.5039, g_loss: 0.5469\n",
            "Epoch [91/200], d_loss: 1.4379, g_loss: 0.7450\n",
            "Epoch [92/200], d_loss: 1.3353, g_loss: 0.7661\n",
            "Epoch [93/200], d_loss: 1.2204, g_loss: 0.8602\n",
            "Epoch [94/200], d_loss: 1.1968, g_loss: 0.7802\n",
            "Epoch [95/200], d_loss: 1.2514, g_loss: 1.0675\n",
            "Epoch [96/200], d_loss: 1.4747, g_loss: 0.3676\n",
            "Epoch [97/200], d_loss: 1.8836, g_loss: 1.1783\n",
            "Epoch [98/200], d_loss: 1.9153, g_loss: 0.5102\n",
            "Epoch [99/200], d_loss: 1.5352, g_loss: 0.7352\n",
            "Epoch [100/200], d_loss: 1.3547, g_loss: 0.9953\n",
            "Epoch [101/200], d_loss: 1.2415, g_loss: 0.7746\n",
            "Epoch [102/200], d_loss: 1.1709, g_loss: 0.8151\n",
            "Epoch [103/200], d_loss: 1.1704, g_loss: 0.6245\n",
            "Epoch [104/200], d_loss: 1.2398, g_loss: 0.9490\n",
            "Epoch [105/200], d_loss: 1.4216, g_loss: 0.4672\n",
            "Epoch [106/200], d_loss: 1.5611, g_loss: 0.8131\n",
            "Epoch [107/200], d_loss: 1.6457, g_loss: 0.4904\n",
            "Epoch [108/200], d_loss: 1.6105, g_loss: 0.7096\n",
            "Epoch [109/200], d_loss: 1.4141, g_loss: 0.7864\n",
            "Epoch [110/200], d_loss: 1.2794, g_loss: 0.7975\n",
            "Epoch [111/200], d_loss: 1.1359, g_loss: 0.9539\n",
            "Epoch [112/200], d_loss: 1.0684, g_loss: 0.8757\n",
            "Epoch [113/200], d_loss: 1.1377, g_loss: 1.0301\n",
            "Epoch [114/200], d_loss: 1.3770, g_loss: 0.2895\n",
            "Epoch [115/200], d_loss: 1.8977, g_loss: 1.8988\n",
            "Epoch [116/200], d_loss: 2.6247, g_loss: 0.4181\n",
            "Epoch [117/200], d_loss: 1.6476, g_loss: 0.4820\n",
            "Epoch [118/200], d_loss: 1.4731, g_loss: 0.8428\n",
            "Epoch [119/200], d_loss: 1.2372, g_loss: 0.9287\n",
            "Epoch [120/200], d_loss: 1.0960, g_loss: 0.7435\n",
            "Epoch [121/200], d_loss: 1.0043, g_loss: 1.6481\n",
            "Epoch [122/200], d_loss: 1.1938, g_loss: 0.2374\n",
            "Epoch [123/200], d_loss: 1.8281, g_loss: 0.6935\n",
            "Epoch [124/200], d_loss: 1.2041, g_loss: 1.1778\n",
            "Epoch [125/200], d_loss: 1.5426, g_loss: 0.4610\n",
            "Epoch [126/200], d_loss: 1.3250, g_loss: 0.3552\n",
            "Epoch [127/200], d_loss: 1.4508, g_loss: 0.5041\n",
            "Epoch [128/200], d_loss: 1.3936, g_loss: 0.7315\n",
            "Epoch [129/200], d_loss: 1.3573, g_loss: 0.6978\n",
            "Epoch [130/200], d_loss: 1.3303, g_loss: 0.5964\n",
            "Epoch [131/200], d_loss: 1.2894, g_loss: 0.5357\n",
            "Epoch [132/200], d_loss: 1.3315, g_loss: 0.6939\n",
            "Epoch [133/200], d_loss: 1.3139, g_loss: 0.7332\n",
            "Epoch [134/200], d_loss: 1.3153, g_loss: 0.6061\n",
            "Epoch [135/200], d_loss: 1.3510, g_loss: 0.6865\n",
            "Epoch [136/200], d_loss: 1.3598, g_loss: 0.6491\n",
            "Epoch [137/200], d_loss: 1.4266, g_loss: 0.6622\n",
            "Epoch [138/200], d_loss: 1.4649, g_loss: 0.6048\n",
            "Epoch [139/200], d_loss: 1.5124, g_loss: 0.7117\n",
            "Epoch [140/200], d_loss: 1.5146, g_loss: 0.6777\n",
            "Epoch [141/200], d_loss: 1.4253, g_loss: 0.5909\n",
            "Epoch [142/200], d_loss: 1.4028, g_loss: 0.8399\n",
            "Epoch [143/200], d_loss: 1.3192, g_loss: 0.8718\n",
            "Epoch [144/200], d_loss: 1.2481, g_loss: 0.7512\n",
            "Epoch [145/200], d_loss: 1.2568, g_loss: 0.7231\n",
            "Epoch [146/200], d_loss: 1.3001, g_loss: 0.8273\n",
            "Epoch [147/200], d_loss: 1.3669, g_loss: 0.6824\n",
            "Epoch [148/200], d_loss: 1.4961, g_loss: 0.7444\n",
            "Epoch [149/200], d_loss: 1.5742, g_loss: 0.4227\n",
            "Epoch [150/200], d_loss: 1.6427, g_loss: 0.4733\n",
            "Epoch [151/200], d_loss: 1.5722, g_loss: 0.6688\n",
            "Epoch [152/200], d_loss: 1.4750, g_loss: 0.6730\n",
            "Epoch [153/200], d_loss: 1.4173, g_loss: 0.6395\n",
            "Epoch [154/200], d_loss: 1.3483, g_loss: 0.7003\n",
            "Epoch [155/200], d_loss: 1.2877, g_loss: 1.0045\n",
            "Epoch [156/200], d_loss: 1.2500, g_loss: 0.8173\n",
            "Epoch [157/200], d_loss: 1.1888, g_loss: 0.6935\n",
            "Epoch [158/200], d_loss: 1.1966, g_loss: 1.0370\n",
            "Epoch [159/200], d_loss: 1.2851, g_loss: 0.5898\n",
            "Epoch [160/200], d_loss: 1.4304, g_loss: 0.8683\n",
            "Epoch [161/200], d_loss: 1.5759, g_loss: 0.5223\n",
            "Epoch [162/200], d_loss: 1.6445, g_loss: 0.5648\n",
            "Epoch [163/200], d_loss: 1.6269, g_loss: 0.7630\n",
            "Epoch [164/200], d_loss: 1.4974, g_loss: 0.7245\n",
            "Epoch [165/200], d_loss: 1.3252, g_loss: 0.9831\n",
            "Epoch [166/200], d_loss: 1.2385, g_loss: 0.9173\n",
            "Epoch [167/200], d_loss: 1.1361, g_loss: 0.9353\n",
            "Epoch [168/200], d_loss: 1.1473, g_loss: 1.0224\n",
            "Epoch [169/200], d_loss: 1.2784, g_loss: 0.5669\n",
            "Epoch [170/200], d_loss: 1.4827, g_loss: 0.8095\n",
            "Epoch [171/200], d_loss: 1.6135, g_loss: 0.3889\n",
            "Epoch [172/200], d_loss: 1.6331, g_loss: 0.5180\n",
            "Epoch [173/200], d_loss: 1.5621, g_loss: 0.8169\n",
            "Epoch [174/200], d_loss: 1.5205, g_loss: 0.6330\n",
            "Epoch [175/200], d_loss: 1.4007, g_loss: 0.5771\n",
            "Epoch [176/200], d_loss: 1.3474, g_loss: 0.7208\n",
            "Epoch [177/200], d_loss: 1.2722, g_loss: 0.9789\n",
            "Epoch [178/200], d_loss: 1.2322, g_loss: 0.6335\n",
            "Epoch [179/200], d_loss: 1.2336, g_loss: 1.1720\n",
            "Epoch [180/200], d_loss: 1.3609, g_loss: 0.3890\n",
            "Epoch [181/200], d_loss: 1.4536, g_loss: 0.7954\n",
            "Epoch [182/200], d_loss: 1.3581, g_loss: 1.2513\n",
            "Epoch [183/200], d_loss: 1.6657, g_loss: 0.4461\n",
            "Epoch [184/200], d_loss: 1.5128, g_loss: 0.4029\n",
            "Epoch [185/200], d_loss: 1.6377, g_loss: 0.5985\n",
            "Epoch [186/200], d_loss: 1.4189, g_loss: 0.7660\n",
            "Epoch [187/200], d_loss: 1.3781, g_loss: 0.8620\n",
            "Epoch [188/200], d_loss: 1.3028, g_loss: 0.8526\n",
            "Epoch [189/200], d_loss: 1.2572, g_loss: 0.7915\n",
            "Epoch [190/200], d_loss: 1.2525, g_loss: 0.7418\n",
            "Epoch [191/200], d_loss: 1.2628, g_loss: 0.7537\n",
            "Epoch [192/200], d_loss: 1.3235, g_loss: 0.7051\n",
            "Epoch [193/200], d_loss: 1.3946, g_loss: 0.8724\n",
            "Epoch [194/200], d_loss: 1.5548, g_loss: 0.5275\n",
            "Epoch [195/200], d_loss: 1.4913, g_loss: 0.4506\n",
            "Epoch [196/200], d_loss: 1.4632, g_loss: 0.4585\n",
            "Epoch [197/200], d_loss: 1.4280, g_loss: 0.6271\n",
            "Epoch [198/200], d_loss: 1.3421, g_loss: 0.8061\n",
            "Epoch [199/200], d_loss: 1.3276, g_loss: 0.8316\n",
            "Epoch [200/200], d_loss: 1.2789, g_loss: 0.8222\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8c0lEQVR4nO19yZLkSJIdDIbVl1gyIjOrsrqql+nhyAz3Iz+Mv8Dv4oWHuXAbctbq6unOqtwiI8LDF+ww44EiofpeF6W9KEKhkNB3coS5A2YGWECfqepTF2OMicFg+P8a6f/tDhgMhv/zsIVuMCwAttANhgXAFrrBsADYQjcYFgBb6AbDAmAL3WBYAGyhGwwLgC10g2EByM794r/7D38Nx6HKnz+nBwdt/qJ4/jz6R7zgES8ZT8fnz+7K43lO8t3pBv8n+a7E/oxr1RigLa/xu+PcyO+mGb87VHKwPUDb3GEf8ms57/Suwe9ml8+f400HbRyL6A/qmuMIbWkmcxA99sddvsDje/k8bCdoKwb1vUe8RvIlznvdSAfnp3toK7/EazaPMtcpvTfKK5nb266HtskPcJx+lD6c8h/wuyc5T/nwFtqGbAPHLsp5N9kFtB1OMn9hjc9s6q+w707Ne/sEbf0rPO/gZL7cagttfryWa7YraJsv8DmdbqVP8+EIbflarjFMeJ5/+y/+PPljsDe6wbAA2EI3GBYAW+gGwwJwNkd3PofjbGqfP8ekgrYwC4/KK+QsSUS+nFwLXwz3BbaV7+T6p1fYNiL3j4lws9Rjf/oBOanzwrG8Q67orx7k4PEEbXO8gePh7U6u+RLnJ8/kvOEBrzEPOAexF27mb7GvoVV7CMQj43u8fela5qTc41ymG+lDvES+XDTI0ftP38t3c+xr+h3uE9S73fPn+RqvGb8X/tyUuBdS3eI92n38/Px5W33A/jwIl+0y7Hva4P7H5GVO6pz2Rk7Sn8G10DZkOK5J7dUUEe/txWecr6Pi2tlIz4wadxrwPMkeN2taJ/0djvhdfyfXLC9/esKpvdENhgXAFrrBsACcbbonDk2SsBYz1l2iKyfpxcwOBzRbtYmdJEkSGjGj0+wB2vJerjGg9yqZrl/jeYMyva6xP66/wu/uxZyKazQFY6vMovglXvSCKIAyvdIGKcrk1TWKGtryCv+/zo30Nz6R+ftSvptNbBZ+wv49yjXnjr777e75c9Wgu6jJ8bv5TuYkndEF1L1GszV+FNqRfcbvhlnmp1ujKRrfoflZ5jJuN+Az0irPaX2H5xlWZMY2co921RfQVOVCbVK06pNQ4h/WtcxJ2eOYH/0ejuMsZn7evIQ2f5L+rW5/D22H79FNFz/I2G5X6NY8vPlKzvlA6+0M2BvdYFgAbKEbDAuALXSDYQE4m6PHlEIGD8KF4hO2xbW0uQrdKA4pcZJdCreO5G7oleutPCI/nk/Ib4LaJ3DkzgoU8ZlfCeGf92toG6O4YMI18feP2D+XSyhi2OE13TfKrfKAHD30yL/SUvjpVGJ/soe9akNumP0Dcu24/+75s480B50KoaS7np7IBVrJ3E4Z9iel0MxM7XGwzmi+Es4+tnTjHXZi6IUTv/foiqtP8nwN5MrNPfLnJJPn4Msc3XTf9/LbTca/w3tUznKeiSjxzYiuuX4v/Wtu8Jq3k3K9/SOdJ8MH860OC85xTW0e5V4fe2w7B/ZGNxgWAFvoBsMCcLbp7h25ApTJzWaP24ppkX5Gc3Ny6Cdze/lttkUXx5wok22iyLxX6BJKj1dyjX6H17jGa047MdscZcXFg0yJe6DopOm/Yh92ksnVv6SIv50+RlM9kOsk/SRz4E7Y5tK758/VPc7lPJHPcVTRbxHtzbxQmYAes5/ySxxnUFSjmN5D2zSTCa5+OjboGpyD3L+iw0et36DZms0yX8GjadqeZFwzRTIWDZr58yB9bwI+T6WKqtvPSAGGiOMKai63tzhf6wfKUCvleZof8J6kG3m+Dphol8wz3s+ql+v0n5EGXVzJ857jcjsL9kY3GBYAW+gGwwJgC91gWADO5ughI249iJslrpGf+ka5qEbkUEmCYYnJIFlL6feUbXQpv12vkP81HfKmpBIO44lvuU+ULbYWjhUG6l+v3EU3yKHcgH2YblXGHJ0nfff38vn6K2zb7/D4Tni4a++gbVSceBpwzP4K/08HrSJD7qt5/F+HmLoe9ztiIi6hOSJfdhmGuQ5qTlyGvHIaZZ8gZLwXkuBxKX0YI7m+vPzWka9rDHhNH8X11ZIiUExlTkKGbsL5hOPsVJjyfMQx37zE8Ot9J4PxW+TvWn1pW+A9CQGfmfWVPP95gv1p9tIHvyZ/8RmwN7rBsADYQjcYFgBb6AbDAnC+wsyB1DE2iq86ihF81MquyGvdZ1SGcS+Fp0y/p+/u5Dzli0toa2rcMwgfFM8sKHWxwO/6vfiqpyd0bm4u5TzuAbnZifzP2lftPiPfKpXfOP/hO2gbHPl+Ozn2Hv/3Zk72F7Ia74HzNM6N9O/UEO/2wl19jvPsJwo91reToi2ziRRnVAhqnIkvl3LNtKf9DQpzHdS+gCNuHdT7yNWU4jszn5c9jTDhdwfF30OgVOZA/TsJX76nVN3hEWMLfn0l6dTlFvnzk5N7sjlif9Ic07IntR+ycqi2G5QCTvb5/Ozy52v95F8YDIb/52AL3WBYAM63AWYy3VXso/9MJskrJYz4ATN9si2Fih5VBtgLdHmE/dfPn+9IBcXfk1vstWRZxQ5NSFYPCaO4+NI9FgRoW+l7domZW+XwEb9bqOw6Mn+Ho1zT9+imqz2JYCr7ONZoJs6Zcl816DbMEjxPp0zMnCzaYZS2SEUixhVes5jVuAPePxfxXo/KhVbkVHyiV7GaAV16OYmNTmr+xoSoYCptYcKBZQk+B3GSuXT0yLqgxulIKSfFOSgT1R6uoa2PGKb8tw9CR68yDK29eS3utn4gYUtyIxaFGneH8/VmJco1d69MYcZgMPwIbKEbDAuALXSDYQE4372WkrtGpZfO/Qf6tg59RB6Zj+QSeq8KHmboQouVtBUN+nn6a+RCSSq/dTO6LXxDbrFUivhFSo1NlTJLfEJX4JQi6Vu/E841BOJfo0rVJfdMEyn8UhUdiDvkXyHq+cO+vtqQQmtUKbYzFTx0qvBlSfeECk36KFz7SCGvecTvForODw+kMKN+O5Oiyzzjo5epcNCUyHVQYa8VhdkOtDeSpjLuQCmts3bT0V4DuwYHxbXTQKo6XIhB9alpcJ+i+EHcYquvMNX7lsLKm8er5895ivtDT1eyV1JQKPQ5sDe6wbAA2EI3GBaAs033bIduqPFSooGo7kKS3ogp4z9gNtZcUT1r5XbxBZlsOhHoI7nwCqoV7sRkSk9U2GCiVCnlLik6ygQKKjLuA5pWsUazOrZiSs8kUhgURTmQYkpJ0VxRm84ZZYu1YjYGivi7O5CbRZnH6Uw13IK0hZJNfnKhadfSjGb+NsXz/htlHX9LkXvfq6FQvYQ/+EumstBGqm/vVXTgQBFtnuZ9UNFlObkf1RQkOUVz5hUJnHoZ54nctTOdN0uFtnUDjuuxEkq52eE8n7botnO3QocrUiUqFJU4cPWJM2BvdINhAbCFbjAsALbQDYYF4GyOfrHHYgHHd8I3yvUOv3xQip4b5CUT1Rx3neK5pBaST0IAR+qqP1BxhbvfysGMbrotZU71QYrdDSNxPlVhYi4ojpQ4sVOctA7sPlKZbVT8Yia1FafcST5SuG6hiixSgYRIrq4hk9+mpF5SZXKNMXIoLfZ9VOGhKfV9N+J3/0GFbf7qC+zfq3u5Zkcuxs903re6YEEgvqz2X7KcMsmo0EGtijQMVKs8V6HHfcS9opL2BUq1F8HZhlOF8zcqBdvkhC69ShWR2CdvoO1nF9iHIRG1pUN1A226rmO2tew1g8HwI7CFbjAsALbQDYYF4Pwii0/ItYtJwkOjoyKCR+HzcaDUxYqKJY6Kg4bfQlvYacfnFZ7nDjlVXMvxasbQ1WNKqrC9UnRxmDaLaZAUGppi6GGhwmW3a1IRXcm4xoh7BOmAXHZbqhDKFEN7TyfpQx05TJNzUVUoLf0Lb6PMTzHh3CU5FY+Mas+ACvrNFc7Xb4OqRHKH97ZW47qk81yRSu0n9VzMPc570GOh0IGSQldjKs9pTmo0o6ri4gLGSHSOwpTVXAbysYee5l2F2sYU+9MdVEzJBaYrP37Efa/Xv/zF8+e7GcNuLzt5hvKK4kLOgL3RDYYFwBa6wbAAnG+692hKrJXrqR0xBnalCtg9UL1vN1Hoo7Ixi0c0kQ5rcZPFBq9BHqok24mJ1JLbKXMkNqjCH+mryYVy+zQU6nhJQpdvXsv0FR3Sl0plr91tcO6GjpRFlClfDmib/uJC5s+RWXjXIZ1qlPnryLzsTtLX9AKvsXcc0qkmxbGIIhwmUyJ9ui8pS7ARk/b7HJ+DhItsdMpV6ci9pvrTsDhPwHvrVciuz0l9xqmiIxRqzG+8qVTz1eOgpxXOSdqqzECP7jXNWCKZ6he3t3C8U+HOF1TUNCRK9NJTEZQzYG90g2EBsIVuMCwAttANhgXgbI6eZpxeKvywJuWVfhautj6hK2BYkWtC8a+2Qo6XN+KOCKSumZPbrnPiLnEUQukSUiFRRRICy6V24j76+g2O68stHr9eSR5tV+P8vEiFq70M30Db8QLTaKeVpCtmtIcRlVuFlVRvH/E87YX0LyWu/5v3cquHgLe9TXEO3ChzOTm+X6QQpNJxJyqoeanSapse3VkjF59QbjK+J+OsXbB0L+ldNRUyfwUVw0jVOPuZXHgRn6/QKNXclPYBDpS2qjZ6MuL+U672gw44rt/f4d7Niyfh4Vcv8P69+Zn0Ic6mMGMwGH4EttANhgXAFrrBsACczdE7qpSiPbivyBkdvYSgjgWFjXq85HFQvs0Guc9K0V43ot+1Ia7mM+X3pD2DOCE/rBV3a/G0yddK+ucXG+RJBRVZLHMphFdfUZrjQckCrXHML1a/hGOXin91fCKZKVX0PgRyYhd/Aof+JMUcpxfoay1KuX+f3yGPXHmMUfikwpKnI4a8OvIhTzoM1+H+wlHdo4reKVyNJVXppp7SZvNchaPOGDvQk8xTqnzwM6m1FirFtSf12CQjNWC1F5FTau5I0mCzSnH1I97roPYCqgL3lVoKv35QhUN3D/hgPjSyGP7kVfKTYW90g2EBsIVuMCwA54fAssppL/8j7iY0kWqloDm2pJiy2cLxRRTzpSXXzdCoLLOMlTvQ7OlSOZ5TNO+SiMPslZn/hgoU/OyNmEzFiuq1B1T9GG6lT2uiHdOX8tuUwhmzARU+g8qKKwp004W1qun9QPG6rFRTi5vuRJl3NwdxH81vdtD2+AEpycUkc3B6gfevb/E5KJX7LVBWXFrJNX1G7ixyNYVCuahKvH8+kVBRSuBLVpQNqWu7dz22OeUC9eTCiyM+X9qdHEs0owOpyJQqjHugGGEdkjvWFHZLdLR+lMH1dJ4npdT7wwcqWn8G7I1uMCwAttANhgXAFrrBsACcLydJ7ohMq5OW5FZ5Eg6xouoifYNhf63iy0lEXjIXKiyR3CjTHxTXU6qdCXKYtcPwRj8Ix/qTn2Pb5gtxi9UznicbqbBjJfwrJKg8218K/0obnOZqRDXQJt09fx4D7gvUUdxiXUmcs8H5WqlxZ1QVZPpK+p43mB7583oHx3+nUpJXAceVFHje0Kv0SQpTnkfpb+woVZg4+6pX6jgDzvtYCfG+oGowHan1xETm5JuIPHzn1X6HpyoutI9TJPKMDANe01Ohxy6RcVKmbhJUKnE64bgouDjZqy4VAfegKqUqfD/SHtQZsDe6wbAA2EI3GBaA8+ujTyT6vxUTqUPhjCRuxETZs9k8ocFSZWKaNlT32auIu5kyiIqSRPVV9lM5UfRWgdTiCy/urhtSM/GqAEC2xr63WxJ11MojFZqC/k7GOV2j+syBXI6piqLb3mAE4ngSN8/k0WTbviIVkjs574oiGYend3K9En/3wwopwPVRXKCPHc7zpsCxDF7aiw4jEJ+UO8l1eM2cXIxdJX33PT4HoZGxHBIMC9vSvf5TFXm5puIhf6nog6fCkvUKr9mdZE4yh5Qp0rKp1LPZUZGPWbnxho6oKdeMVz+dApnnqcx7QXT4HNgb3WBYAGyhGwwLgC10g2EBOJujr1fEMxvhEHOK3GMbRPEzUHjsEJHDDGvloiJXSdornlmim4dDDTfz7vlzS8URt1SY72dfCV8svkTO5ydRtZmaK2jLKOw2VJK9ljocZ/5KNi7yPXK8cY28N1XFHuYS++O8nGf9ifjfNXL98kq+uyNXTh6/fv68GjCE84sHyqJSWXvugH1/cUthrhfiqvvu23fQtjmpcGJyp7XkJtN7EcmaC27IWEKB7tmeilImFzKWv3ykZ08p86akcNPjlCTrjfR3JG7dk9s1n+S7KRWmCJnM38TZdOQGnpWCUJpiKHSiFJTyDu/XObA3usGwANhCNxgWAFvoBsMCcL7CDPmbZ5V+5+j/RT/p9FL0B+YU+FfP0oU9tcVZePk4I08qeuRxrRM/qA/kR6c9hL/7IIoqn++QO/5C+byzK0pPHPG812pPYV2hn3gaxRcdE+SVyYQ+926neFyOnNOpKjjNCblZ+rCD4yy8fP7s3We8hqpo0hbYlq2oAKJSoHkg5ZyPe9wrWSmqW+c4P4dB+ttRLMNNvILj5lqueSIOGgrZewg79NU3JXLtvz5IO+nxJEklz1c+4jMbHT4HR7UvsKWU35ae00HvAQ20pFR3HYUwD7R/VVWqT6Sam6byzJz8H4zsj8Le6AbDAmAL3WBYAM423S8e0ezwStFlVaBv4rMKQ1zl+L/k5xmara1yoX0u0fTLlWDgiqjDSMKNOmKwTtAk+vUXV3B885WY+T/8gP37T93b58/Xv9lB28UrPM/FXn7bv0JzzivD8dii6b7/hG6oxycJtb3bvoU2/1FM3pkKEuxJiLDYCNX4IsUa8V0jVKIgcUN/RIrypEI6N+Sme9sRJVCPxUTFI50u/tBjIQ/3GosuXp3ku37G8N2jCnNNPbkxSXSnV1loPsFxja36sifFG3J9pcoVFzIqEEkClQmEsuI101rdI6KfI2VrJtr9F8gFq4Q3Vx6zD8+BvdENhgXAFrrBsADYQjcYFoCzObrPKRRScdDTiDzuZ0pxgwsbDMQrfz8LPy1TKlBXCN+pW+RQScR9gVYV4psi/v96c3UNx9qNcXuJoYZPa3GvfXyLhQ0eqGDkC5Xa+HrCNMfHC+HI1xF/18zoPnr5lXDQr+craPN/IXN5mFGF9nKN3N9txfWVh38KbaEVd83j599B29RiOu74ION+31MxDCoiMbVyH8JMKkBO3GJxwvv+sEf30bVKIT16DLeegMsStyZB1FH1ZyCFmVIXWiA52ZnOO6kQ2QM56lIqHhLVHsLkSIlJuXqngM9lTaHQWhTWkUptGqRx3+MexjmwN7rBsADYQjcYFoCzTfeRsrN07bMLclVoMcbdE4kJUp02pzOpHLoxooo8Gwo0dycS6NvsxLSZVuji+M9/9Vs47n9x9fz5l/Me2oqT9PeJzLBNj/8Xi0ZMU1+8gLabWY5HEhe8vbiD452KJIwex3l8FGqxeUL69OErMqPfaYHFH6DtzUtpq0rsayw/wvGflNKH8jt8RH5DNORJuYFiwXXHVL12h27VfOYoOvlulqFpqm/1nFMGGGU8TqqGGj0iUORjipSBRi6zVEW4uRyjHud8hydWpnvh6N2p6rbFkucH6adTlDN1HI0n464cpdqdAXujGwwLgC10g2EBsIVuMCwAZ3P0a4cuqpgJz5xnPM0npZRRU+gqF3C4UAUcuhR5m3Z8tTP+bjOQ6ocqlthRAYD7Ar/7Zzei4rIqsY74P3z4q+fP2Q756OAxc6u5kf+TYYX/M9NCOFW8w8ytpsd9gfriSs6zQ47+dvjvz5+vvvwK2k4R78nHw0762iJH//ZR7sk3wxW0uZfIT6MqxLAmMZN0Zp4p+wZ+wD2EQu1xzAHdaacjcu11EF4eyAU7qzjXFxN26Fcv8JrfHeW3pxGvMQ2Kv5Na0Eguz7BR9dEHfPYKUoW9XauQ6p6yD9Ve1hp/lozkjkxTGWfYklrySdePx3VyDuyNbjAsALbQDYYFwBa6wbAAnM3RTw4ra2xVWGBLqYz1pZCRixZ940OGRKWs5TybgaqLjBIKOZFfcfSkRqOGkiZ4njfxNRw3d+I39lT95FUvXG17i+MqX27h+PWlShcMqN562v2j6jvuEWRr5NaN8r1efo3z9dX9Xzx/7gbkkT9/gecJzd88f77vMbS37GQsnxMM7Q0P+Bi8Ub7y0aMPuaYw5UKHkpKPO1FqvOkR53mmd8xJ72mQyqpTCkH3JfqQrxOck18q5aPB4x7L36trtvQchhn743sVH0AFPZMNjrNvhJd/7bA/rdqn+BjZb05hrmovKTxhf/Q6mWgP6hzYG91gWABsoRsMC8DZpvuKTJsHle1zk6NZ5g5i2pwiulXWpG4yqaylexK9y5VY/8uA2WEHCk/9rBKwVjOaRDevMTvr1xsV3lheQVun6EN5IiFLj+Z5qup2zwd0wYRJxrIasQrl/AIVQuqdzMFIBQ+vnlS46is0ucsndNO9qWWOfvHneGsfg6qP/ju8B6HAuV2rOfhI7tFvqOjA0AjVeKKsrit1H/YXOJf9kVxEvfx2m5FpnEj/5gPSsm9bfC7XpZjRlw6vUedynilB092Rq2tUh0WB54k9F99UoccNUYJafluTe7Y/7uA4SZXAKYWVz+o+hJT8dGfA3ugGwwJgC91gWABsoRsMC8DZHH1LihypUvh8IiH6oNws5UR8goop+EQ43uSRc66DXOM+pUIQa+S5hSr6eP3NFbS9oILz6ZW4YPoK+Vf5Sbk43nwNbZ5mKyj11On+v+F5VPGJUCOvTY+4b1GuZb76PV7EF6JUc8jQvVd/gy60m07mJMZP0Hb1QfYQmi++hLb1iPPejbIvULR4v46UslmH3zx/zgJy0O1WzhM/fg9tM6Witkp59kgprJVSexkLfA6LCeeyV0pE73OcnyqT71aUpjpxuQeVqrvxuIfRUxFIldmc7GnvyE9qb4TmOQ30QOm1Qmnh5Un6N6Y//f1sb3SDYQGwhW4wLABnm+6HArf7n5R8x82BXAFKQLBFqzlhAY6gBO9DQFPrcyI20SpiVw87yihS4pX5A0aX3X6DZv6oznvZo6nlbsX8jKTb3z2SC60Rszqs0RT1rbh5wvwzaFtvyGxULrWKKEmmssU2LZp+M0WQeV0IjUzumIqwZLnFKMdD9gaO3SjjrL/HbKwfih0c142YmMUOI78enyQCMSYk0EkZYC6Vvrc5jmtWCjx1RPrSUl07XbRhIiHLUT3uLRVP2CQk+KhEHXcVPt/XNT6nOoPPUU3AaZT568lUd1SQI1G1/RxF7nWZEj9NLTLOYDD8CGyhGwwLgC10g2EBOJujN44K+qlsqA+kruI64T9DScodE3IYX6rCchF57qgo35GKNToi+73aQygrvMbjBfKvF6qWed+g62TeqdruFEpbRnR5jLmctzhieGyrMsT8I3HiBrPptv9aOPyckdrtvewDTFSgMqeQUz9I312DfW9vxM1TnbAQREEZfKNS82luSMH2dxjO27Wqvv0lhX+qOo/7iHNQUiHMUYWk+oD7ArNSk60mzEgrUpyDRt2zwjPvls811Th3lHW2Vi7ZibIh+49UvLFWSq8pjsurrMt0jc/s1CD3j4qzz6SWvE7VuP43Xs/2RjcYFgBb6AbDAmAL3WBYAM7m6PuRKs4rV16glD8dTTj1lOKXoQ9wPKqCjA7T/6IqHE9XTwpKK+wb8W02GwqhfCDV1ddX0r8L5KfH4e3z590jXkOrzyRJkrztlM92wFDMX78Rf295jQ75e/+P2L/TL+VztYO2+VqF0gZs83v0Bc+KW6cXpI7TyZ7BWFMhwAP2fddI32P+HtqaHMfiKpm/+SMpw6jw4nGPPvYpxbmsFSduKPU0JvLbE6WwVhRfkakQ1ILUZFOvinaW2Nec9kb6VH7re3y+j5SyPe/VnsIK+1erIpU9rQXK+E28Xo6BYlN0X0csQnkO7I1uMCwAttANhgXgbNM99KQsokxDTwUB0kxcKT0JI1I0YdKXYrL1A5n5azHZ3IimVUdZQmvlbts/4Hm6L9Gd1G7F/CxOVGQxE9fbrwo02fo1Zn39s0nGuScBytSJSXvoMUzzcouFGPJcTL8hw1uSq3DUfqbiAGsc53gvv62fkAZle8lmyzZEe/Y4t035WQ4o064js7rbihl58Ni/pyc1bkeZdytyLanzssZkUPc2omcr6fgJ9nLPQoJ0IZRy4nbE+TlS8ZBkVJRkIjOaCjJGlWm2GrCDvaoLzy68rWeXsdyXENGVOqpxZYFis8+AvdENhgXAFrrBsADYQjcYFoCzOXqfI4eZj/I/wk87aIuVCgmkdMCJqtPXqXCR6gKvsVdqmzOlNfoZOczRC1csA4ZpfiK1lctWuNKUkwumk7a5xtTK5JZCRRPh7PX9O2hrlcuxeIP8NJILRkc7RlLrmfcS8pn16FYZKSS2WElhxWGPcxBXMq7uM/LI+QWO6/I72X85ZuTCozDX+kF4edNQSK4Kf2ZRlG2J3z0ppdVmR3OgQ4898u4qRf4clMJMzIlbt9L3DRVZfL1Chd3v9so1SNw6SemZUcUljxEHWtRqf6HFMT9FcgMr9aW0okIng1LCzbCv58De6AbDAmAL3WBYAM53r1HkkNdWR43mXatcZhVlvY0nNBuDslsb9nDoEDty03nKgpsypehCIUf7geqcfy9RdLue1EOqD9KfiBlpA0Xj5QcRRhwvMMPpQpmxefcNtDVUK7y4EGpRTug6CaWY/acNUpB1xMw7Ja6SxAHpQl7I/Ow7ul/f43m3KlOwoxp3IaK6yqdO5jb3eE+6RKjXJbkC9zgFSaai6CIJkSZK7aUoKPqO31VrlQ054LO3zmS+akdCpBukRb96lPbvKNOup+w6p2rMeer70Km+r0gklJ73QbGS1NFzoOrIzQlStnNgb3SDYQGwhW4wLAC20A2GBeBsjp5SltCUKk4ayf2gxOYPHt0EVUkcfRaR/25ELq2EL5NAXZ1L5Jm1UiE5Usjiw1vkY4fXwtXG3SO0/XtV8DBd/0doe1Nh6GosJbNsoAynzVHG4i+QA39VIH+eVIXIcIHuozjJcV1QqGqL3LFQdbvTEf+HH3IhxXmNHG99i+G785PMbfke5+7Jo1LMrFxYjnilSxQRPyLXnymEebgTjpynOAe9UnBJG5ofT65KxZ+3a+TEf/GFfH74AE1J+oTP5aVTBQ8TJNNFhfN3o1zIpx7bWvVbT/crpbUxKpUk32DY9KgKpvj/jfezvdENhgXAFrrBsADYQjcYFoCzOfpI/sG8EL4xdFR1QxWlq1LkPnWPPO6kVDI9KXpGFS4bBuQ+F1QM/uVaeO53JwqvPCGv+zAI/7ktkTv+6aVwqqnHCib5jCGx/VG4d+zQD9spldyh+Qxt7gOqttT/RPrzqkE/tUvkt21Ajrfd4Jx0ir+mPYbAro8qRZOq5xx/wOOTUloNtP0y9Fd4nMtezTCjc9ypVM9jhY9ae6DilkratAvYwUJV04k5dT6SulEmz9Nxws7/3Z1c8zXtK92dsD8nVSklH3E/KND78YXi7FmOz9pBVbJJtniNr8mP/qgUdTsKPXaVnGfs0I9/DuyNbjAsALbQDYYF4GzTPfckDROU4D5l8xRRTIuxJRdCjiZJmsp3/UThlipTaaYieCcqNHehlD0qMjcPVHDwpPr3VXgBbXEjmUHXGWZ1DQmaw+FWztM9ocmWreW3oXwJbZfkXnMqvLgn0cT1WpnDT+gG2xVo+1224jM6UMhpWsk1CypA0L2kQhVK5PHt8BHaPp2QWhyOSollpJBcJ9fhe7u+wHfM6Unur6fiHFG526oClWG2Eef9k5q/OOIz8jTJnHRUaGFTUQ30SehUR5SpP+Lzfqcer4q+mykVme0R18JNTedV7rWWsvJWg5zn5KzIosFg+BHYQjcYFgBb6AbDAnC+e23m1DjhEJeUvnlUhewd8QlHaY86JTEkpG7ZyG/zFfKbiYoIfq/6s6JQ0YpccfefxSX0w58i5/vnTnhwe4V8tKLU2LYXddmbF6Su8kI43ul0C22rGjl6GKQ/bkbe1qqU25RUTdltN27UXLfI8U5B+hNuaRxU6ODDJwl7/f2Efd1FDBkelRsx9xjCnKrHKye+PAXcNyl17QLefynFrVmQ8sojFdnIot47wn2Kf1XIRXpy4V1Qocmjcu1+i01JSS60RvHwR3InZ0olaaS9h7uJCkEoFaUpoWc4yBykCYYlnwN7oxsMC4AtdINhAbCFbjAsAOenqRbEv1TBwaag0MdEuO2c4f+SNaUnjsrvGCJxD8XVXE8VTGrkoFOvVGkjcrxqg9z/eJCxHB+RgIWvJez10mPIa7rF86yUL9ZREcpRqX9WG+TSE/n1RyfVYYo9poFOQc7TkB+/JummkwrVbElJtVRhk48XyHO/f4/3tm13z5/nHu+Xm/C4UrJG3iMnHtUWx3TE58BRKvFGSY61pI5aqzBbd8JxhYLCU9VlIqnmftfKed/c4jjuZopHPSgOT9VX2gz3dTZqGXUzSZxt5VmsW/TV70nOqlFTkq7xeR9aFZbsLQTWYDD8CGyhGwwLwNmm+wvKzmpUeOo4oysgz8R8CXSFI4UIprMq9kCuuKBM8ECqIzGlWupKzWQ3oWm6ImXVqAT5H3eo5PF3WzHl/+VXaHIfe8qqUtl2vkYV2KSVOchPOAmRFF5cqeROMuzr8IMqwEhKuPkB5+DjWhXi+4Qi/4/qX/rnFs3U5gkpwUelkhIdFUhIsQ+xkuOJXGjhTuYno5Dc0uFcahWXgejd3U658HI0jceZnycVNk3ZYveqUOcTqREPO8qK28i4MlIyzsg1WJUqjHtG81zXqNzP+KyVtG6coiGOKIDL1VxOZy/bZ9gb3WBYAGyhGwwLgC10g2EBONvYP3rkX15xa19SAUStdkmugEj8Yla8c6KwxERx4CriHkEY8ZpBcVs34DVbOtZhuTty+/ztu79//vzDe+TSf/oFhn9OKvyzvUY+eNFI6KjvsS1eUHpiJXsB4R7H2T+pIn3kXjteYP82393Jd+tLaHu3E87nKMR0IG7tVEFNygJNcnJ99epeU93EJBZyj+YM7y3v6/Qq7bmOpBg7y/2b4gW0JSm6R7O1cOQVKRZNhRyXA7npNsitRzXXFxgJnbwaSP2lk/Ne5vhcflBKNvcUtr0jl16pKuQkR7wnJ5UmXha0Ts6AvdENhgXAFrrBsACcbbpnHqPfRlWzOlD0W+/E3oseL+EpKms6arcBmlNVoqPC0M3D5qdX13QlCSym6PqKSq1jQks56R/FLGtSNPkf2rf4ZWXWxt9RBOAs9dLZbGV7uFcZfOsXaHJn6revKUotPWFk1bvs6vnzlwG/u14pdxEVoXxfsw9UxjJQxlxZ4IRFJ6bzFPH+JbOiHUQPuObH3EkfPFG4VLlDJxKDDDNm15VB7vWGsiGjcuG1JAo6tfh8RyVGSoIyCYkJJa/X8ocHqoHuFDXtSDXmcoXPzJMSNfUltqVKoYfqnZ4Fe6MbDAuALXSDYQGwhW4wLABnc/SGFGZSxZnTJ1Z2ldNmpP45PSDByTXXJu7qU+GDr6+R32gunSRJclDUaAo7aPtDXilcLSdOFW6EZxYHbOt7dOV45eaIVMTiSfGoYqKsLgplnSeZg/gJwyS/Vr6dosL+1Gvk869rcaGR9ygZVOhxOqLy7eoOC0p4lZ3lJ7wnE+1bZGpoYaC5LPRzQKG0Pe2x6FeOw7mMipeXFN6c1qS6kyt3Vsch1eqmzOgqTeg5zdQ70Ht8H2YFjvOtDo2OlKXnZL5ycstVFOrbKWWm2JGvUvkuaQrOgr3RDYYFwBa6wbAA2EI3GBaAszm656JvrXCjgQpHpOr/B9GSJKPwxl5VTXEZdico//LLEf8n/eolktC3nXBbEkxJup54peL+nP5anoRLdjP6YetI+wSTSiv0FFI5SxunXaakupqq8MaeUiB/28tgflUhJ//qiPsm7zYyzk2NKay3KkbhdIk+429I6fXzQVWZucaYhPIRYxI+DhI6unY48adU+ucC82W8J14pCQ/k475Wz8FjTf5lUksNo9zP0FOosUqx9RSHUZWkMKNCvFmVltzhSa3Ud7sTNmbqOQgV9n3HIcOae5PS7KwUnTx34AzYG91gWABsoRsMC8D5IbBkgzcq9NAX6BLyvXw3VpSJVKGLKj1IFyaqQ+2UAP/v9iSESNk9vhAz8WsKl/1M7qzTpOgCud7mTkxTn+M12sAZfGIaclZXosKC0wTnwDlSD1H/b1NScNHm5+9b/F11hWZ0oahG/UjXUG46v0Ozfr1mSiIF0/sT9v1Dj+f1lTrOcC6VSEvSM71j958KSy5p3rtEiWdO+KxNBQklqtDffIMU5UvdnwEf/fsZ531UVKKktLzgKMS6V666kQqUpHLsAlKtWFHW4EbmwHXYP8USkxB/+vvZ3ugGwwJgC91gWABsoRsMC8DZHN0Rh8lz5TaYqYBeLlzN9cgjSVQ0mVR4YTmRKmYi/G8qsaufiJplKt01qZEvrzitTxVvnAOeKFduskipnm6m2EMVbhmIvyc6ZZNcJZviBo7bUdRM2oSuqdx/gfZJHkYc2KYSDpj3GOb6oO5f+QX+LiWlk/ZBfvs4vIO2klJay63cvw0VV5hVkcrY7LCN1Ft1AYzYUghsLVw7C5SuPCAnbpW6bDngvB/U9K1LUrMdKHRVzSWHbScZKuyWucyJLhqaJEkyq3Br3vPJ6F6rbNdkH6moqdrX4d+dA3ujGwwLgC10g2EBONt0H0hwP6TKnKLIoUSZ6yOZKwkVaciVC43raCUnRQ8iud4owm5cqfO0dE2CS9X/N8pa6pWpnJE5nhYYKTcrQcORItoKqLeN53lQpnqSJIlTGWKrFKPUklplPxFzeHjADKxeuTIvrvB/+HSS86xI8eaUYKEKH8RMrDM0sf2Ix9e1HGdfvYK24ahUbUgNJ1IKVqX6NFMNt15xr47u7ZYKOqSD0Ma6oiIfSmTyPuD8eM4IU6Kmk0e3oaPItKmT/tZX2L/mJH1fU03AKWD/GsVr0xznIFNuu7Qy091gMPwIbKEbDAuALXSDYQE4m6MHCivVFJQ8JUnw4hqYyS2XpMg9KhXbN1NvUpWpFDv6n1TRd1V2T8zJxUGZUlErhtBpdXcjufSSCV2FmVJQCaRy2qtsuxW56UqPRQgut8LRG+KOg6qlfiDXZO6pCMIHGee7N8hdf63CY58OGEbqSEknpPLdn1NhivsTzuXF5koOCgxv/uYb4bZHUrX5/dMnON6r8N6RFYF28nlLIcIvaN9EJzm+pD2WXocaU0ZhS9mHB/XbTUD32omKLqbqeDrSXo16hmdyz24SDNF9UsU4C4f3Nia7589dRwvuDNgb3WBYAGyhGwwLgC10g2EB+AkcnUIGNU0g9ZfZq1RBCo/NcvThVooL7WcKb/RKwSVlVRZK/cwVP2ypMgq51XVxRFci/6rUhgNlMiaRNgZyxZ+HlApNqngBX1C454Rc9mkv4x4r2tNQp/UFqatQjMLPLoXXnQryw6rihCuqUzjTPXEH2YuoV6hq4xrk/nkmY6kT9KNfqkKYd5Te2jsM8bx9IXP7vsP5uVZVZpoB4wz2R9pfyFRsQ4bnmZWS8dNMvmh6Rspc9iLSAe/7lp7pppD7l5Py7DTIPekT8vnTvkCi+g6fkyQJuVLx7egZOQP2RjcYFgBb6AbDAnB+9hoJGsZSTJSczMSgTYuCanHT/5ZWueJWBZpz3uma3pSl1FDWWZD+OYfXOFFIpS7gUEz43VFZdOlAv+PCgKpm/EzhjWslxjjOeI1A553U/LmeMssuFZ3pydwt8LvlRpm1T9if3YXMX91TmO3tDg5Xav6ah3to+/rLN3BcKWWdu08YSvuhlr4PND/ZFs3P9ijffUEKKicVfl2n6Bo8XZESy17O2yfopktHFarqyb3GGY7qees99jVQ5luhwqhnUo2ZojLXiX52EykGqXHO5Bq8UoVGNrOJQxoMhh+BLXSDYQGwhW4wLABnc/SJixCodM45J9dXLxxiSolHkivOVeJG6KmAX63CXln4smA3nS4AQCGnrMKqU2XbDLlQnQlvitMK2lxCBf1U/3LmZmtpmzocl+P/ryrNt6K+j1q4n0IxN2vky+tR0lbniPzUO3ERvbrG/nTkanqKElbqS3Sv3e9xf8F3H6XtCfcQxgu5zprcofUDuqzulQpPT56vlZqfqxTHdbOlwpPKDfwdubpOqm2mUOOM8lR7VXzC0x5U0eJxqtKrWfFmVGow84Dzw2pLyUr6VA3Yn1rJ6A65hcAaDIYfgS10g2EBON+9RpFfqTIlJjK1slpMP9eimTGVGK1Uqsyf2KMw/sFJhFaZU42tAu2ekyrwkJLZk5ArZdYuPfRwJKOquXVB12hJkFLXW9PuxiRJklGb6xOabIGYRKpFDDN25ahxU+23T/ffwvH7RK5TUSRjHcQt9YnS4PrVl3BcHqXvJ8r8O1IN+1l9t3Ro0o7KHci/y6leui7SNyNjSgo1Yasv8Pl59QG/+1bVLo85zsF2kJDAfYLqPPMJn69CUYkpo4xCh/ShVI9bRzXioxZ1pOepaJEWrRR1LRzRROXu+/jTBWbsjW4wLAG20A2GBcAWusGwAJzN0RNS1QiKD2Z/kAgkvCQnxcyZtPB7xW25CEKueMo44kU4S8hpPkYuM13r+n9eSMe5JgSVAdYj15/py6nig+GAA0tVrffZo5KII46XqKy0k6cMvlk46aZF8to6UmjVw8zxPEfV9zDg746P7/GavVwzI1fOQHslSZRw2pZCjedE9gXSgOeJpJrbq6qLNxc4l3+m9hs6Kir+PZ1nf7x6/hyogEPnRYl2fYHKNIcT3uuhl/aKinzMtFcyalkiUsCJ+pmh/mwcKuP6Vs4bKFz2vd674cy7M2BvdINhAbCFbjAsALbQDYYF4HyOTs7fTKvCknzr2Km2C6p20pESy6wKMhakTqrSDPNAoZdEw8OguH5klU5SkVHDnqhqSVD866ZAv/4uoNN9bPVeBFVGUWG3aYZpofOG1F8ehJ/ma5wfPQdJTbEMxE+zVLjbw0hht/cSchqoft9Emyw6DmImf3fg8OJJrpOR+q5X6bBDinPHiqi5Snf9sxSLUKZK1eYTKcp8yvC8w7STvhJfnlThwpZiLTYBn5mylGdxprYjBY4EpYAzk2pMph7pkfzxRwq7HdUelaP07uIoz5OjIqLnwN7oBsMCYAvdYFgAzg+BpeKIvhTzpU8m+q58Trl2OhUrd42YlDMVawzzTr4X0B0yzVhMIZRi4kZKC0obNM8nlVE0kThkrrKP2grNwozqdleqqGFD4/KN9GFLJndDYcFRxVCyXmB7UMX+CrpdXIRA1Q5PA7rwgiq+7WqkJPkRxxmUmhDVmkiSHu9RVMUu+wnn0qkCBRuifpcvMHttGKTvf3OPRSh7NZR+pPBmKhAyqVDRjGqV6+NAFKAh4cZWuUcduTz5OU1U4cmiJUFKnW1HfR/JBC9VQcsjqRJVF9KHFhnuWbA3usGwANhCNxgWAFvoBsMCcDZHzyiUdVBKoY7SN5Nc0u9cRO4TW3Z5yLGnYgHJk/CdKcUwUlaNSVUaZCDF2DFFfu+jpCgWI3JFXUyy7XDMmwsKXT0qrsY8V4WnPrR4nopSfrUoyfyEXD+7VFyR5i6SlyVVrsKpxr7WTlxdk65amCRJuMI5SE/CXzMK92x7fGQqpb6yoXTcdS335OtbbJvItfRXP8g1W0qxdYqTBlLt9eQ61SHXqaPQZ8XniwzDieeI+wK5l/Z5RlLsyR3p1ByNETdZ0iDjdLTcBuLhyVoVWaQiJK6RcXoOoT4D9kY3GBYAW+gGwwJgC91gWAB+ggospWj2wndCTSGBg/DpUOLvao88fKiFO65GTNtr1uLv9eSfTDJS5lR8zBfo/C1IL6o5Sh+aHrm/FnOdSNpqlyKPW7UqlJZ82rGQ86Yp8tOOQii94o6R5YZOmrtiGxdvTLRyadjhNXPhnOV8C21ufqRjJcvVIx9ckRRYPSouSaG1UY3rv/wOuTQV/kmCvg81hSXr20fPoaPQY6+OWQLKZyomYSL+ntBcOhlnRym2JPibOF0YkyrSlCqldORNFdpDmE/S33TGZ1YXGU1H3hT747A3usGwANhCNxgWgPPda5QR1qt/EdmEpkShqy2Qu4HFXq5+KWb2aUA10kK5NfpvfwNtGzKDRmVCDhles9tj8b9RubcyMssGFY4aBzTVU8rSa1SxvUiqLak2x8lM9AnVvoYQSzSVuyjfTSmkM67wuxed9K99cwVtiX8h1/uM81GWeJ5M2dXZGk3uJmDfJ1BAxXvSqhroA2UmThNdcy1uxUDutURRFMeJWxOF5CplnUAhy6OKL/aOFYrwHg2jUm+l7LWkIDdnJ799RVl6XmX77Skc/ClBV2ql+h6pYsmcqDWWckXIPw57oxsMC4AtdINhAbCFbjAsAOcrzFTIw1NV7WQarqBtnQuRaogDu5e/wuPki+fPRY08rv/tJ9VR5KeHQOmuQdwz2Uhht0T5lBBLMpNaqubTKbnMAnM1L9cpakqxVQUaA14imTyGnPoorpzQofvKqXTJZI1jjgN+9zCqfYEfSG63EqXX0JBa64H8Ykp5NhkpFJNUbVp9H2igk3JRxR7fKdGTaq7yruWeihFO0jiW6DodUlIeUueJ9HiX6sYHfiiosuPsJWy6zKkoZUtuO6Uwc9/hvb1WVYJ+leD9eksu4/1axj3TfkxQqbEc9nsO7I1uMCwAttANhgXgbNN9pFAm31/J54jRZSdlopA+X5K9/y0c93sxKSeKKkqVe6SfSAjRodkTvIqIGtDETTMyh/WwqdiDU+6QQO40R0L+uervOFHN7FKOUyryGBKcL/1TVjOp1bjaA47DRxqncsXpohBJkiTjk5zX/8FNwey+YlYZfKSKEqmQR6rvPSmvRBX5FWeicCR+GFQFS++QenWZqhFP0ZMVizqqKM0TTnuSqKKPJbl5e7pmVC7jnpVIyccXW+nfSNl+R0XvHsjtux3xPtz08t1vHQqKlso1N3q6f2fA3ugGwwJgC91gWABsoRsMC8DZHD0diT8X4vqaEszyijqrqiNXAAnnN1EVAEjJxaEyp1JS6ZxndMEE5bLKSuTSA/E4Pyg3FLm+nFKnYXWclDjorL+bcTqWUseh8NgsJ2XcRI5rVoFVobRug/McuCCjDitNeb9DjnOau3nCkFhQ1D1RaG96gONCVXYcSPUnqqKLnl1vCSKosOQpx/dPNspcuhL3JQZWVlXDDqR445QyjHO090DiRqUi+CM9wo4UjEBFpkL+3A3yXL4jdeINhbn+TIV1v6YZ2qt9lXH66e9ne6MbDAuALXSDYQGwhW4wLABnc3RPvNKpooZug6cpWvGtduSL7ivk4UUjFVcmKuA3KF95Sa7Mif5HVaXw6bHjlFGs6uJUaGuW0LhUF6ZICiCk8JKrMMWBuH6pFFlLClnsKFe3Ur7yIXkJbf5CODHzZV8jHxx0CulEt1b11ZHPf6J9AT9Lf3lfIiEfboxqXyBSYUcV/pxSRZyC9i1m5Z8PA6kSBTkeKXw3ekx3jTqtN8FrBrU3MRGXTo6kBqxUXOZAewYUjj2pEFjXseKNPJeRNmDaPT4H/6DmdkV7CCGTtpTVY8+AvdENhgXAFrrBsACcHwI7kGh8LllEoUUlFqfUaLKZlGBYcUa54rKWTD/lUugppDMr0HUyDtIf7+6hbUjY9FK/ZVNUhX86MtEokjUJmSoWQK4lHd04ZaQIQmbjrAQNfSSa0ch3hxrdfckRQ2l1aK8viAapcN1QEQUhd1HsVX8pNNSXVIhB17ufcZxzptR6RiyYUFAIs1MiijWpv2jVn6rH65/IZeWVOk0gBZckV6Z8R6o/ORVvdHKenIt/sq6kDvXNyG2n6F88kooNFd+Mk6gAxeQB2mZV3DIQ3TwH9kY3GBYAW+gGwwJgC91gWADO5uiRKylOogbjPXKhkCoutEJOno8UCqnSTydSFvHKDZWtyWV2ekXnUemugVJGSUUmVzxu6PG7TqWthhW53lpK0VS8kjw5SdR+OlKfySfkq7GS887k+ooqbDPskNNVNF9a+GSe8bu6sEFBbsOJXJ6TckdGLlRIh/pUKSnzqkzdZKK9kEBhromTZ2ig+xdUsZAux7bsD1SAlJJqhjdlpdRyNhmlPVNBzcdK+tPTmNOOikCqn0bag4qaT9P8pKy6o1SSHLk8o4rVLllK+QzYG91gWABsoRsMC8D5kXFUz3pWUVnpgdw8lfz/CBTtNpJCSQhi2mQbcnFcS0GHjDLkhsPfwrEuwRXI3Ey3SC0mJXAYqVhAqkwv11HBhAr7HgehM0WGbqhprcQFKaLuNJDLSmUqhUAZV06LVVKxiQxdcV4roXREMzbSNhwpd+yR6pnV0odiJlfgnszqSiLKYkERd8qsjjSXc6DsOp2lRwKUqVYPmrjAGz57qSqMVvc47y9UhBvXymMppEFxsTBi1Jxj75aiGpFqDUZFTVmoJlBGX6YiKGd6hn+pIkzXf5D798dhb3SDYQGwhW4wLAC20A2GBcDFGFkr02Aw/H8Ge6MbDAuALXSDYQGwhW4wLAC20A2GBcAWusGwANhCNxgWAFvoBsMCYAvdYFgAbKEbDAvA/wC6Dnxg3J+0/wAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}